# -*- coding: utf-8 -*-
"""АТГ_сем3,4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Oafx8frtR6TZLLeahB0TqIpPh4teiuS9

### 3 задание
"""

import networkx as nx
import random
import numpy as np
from typing import List, Set, Tuple
import matplotlib.pyplot as plt
from collections import defaultdict

class GraphDataset:
    def __init__(self, num_graphs=200, min_vertices=100, max_vertices=150):
        self.graphs = []
        self.chain_covers = []
        self.optimal_covers = []

        for _ in range(num_graphs):
            # Генерируем случайный связный граф
            n_vertices = random.randint(min_vertices, max_vertices)
            density = random.uniform(0.1, 0.3)

            while True:
                G = nx.gnp_random_graph(n_vertices, density)
                if nx.is_connected(G):
                    self.graphs.append(G)
                    break

    def find_chain_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит покрытие графа непересекающимися цепями"""
        remaining_vertices = set(G.nodes())
        chains = []

        while remaining_vertices:
            # Начинаем новую цепь
            current_chain = []
            start_vertex = random.choice(list(remaining_vertices))
            current_chain.append(start_vertex)
            remaining_vertices.remove(start_vertex)

            while True:
                # Ищем следующую вершину для цепи
                current = current_chain[-1]
                neighbors = set(G.neighbors(current)) & remaining_vertices

                if not neighbors:
                    break

                next_vertex = random.choice(list(neighbors))
                current_chain.append(next_vertex)
                remaining_vertices.remove(next_vertex)

            chains.append(current_chain)

        return chains

    def find_optimal_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит оптимальное покрытие графа цепями (жадный алгоритм)"""
        remaining_edges = set(G.edges())
        chains = []

        while remaining_edges:
            # Находим самый длинный путь в оставшихся рёбрах
            current_chain = self._find_longest_path(G, remaining_edges)
            chains.append(current_chain)

            # Удаляем использованные рёбра
            for i in range(len(current_chain)-1):
                edge = (current_chain[i], current_chain[i+1])
                if edge in remaining_edges:
                    remaining_edges.remove(edge)
                edge = (current_chain[i+1], current_chain[i])
                if edge in remaining_edges:
                    remaining_edges.remove(edge)

        return chains

    def _find_longest_path(self, G: nx.Graph, available_edges: Set[Tuple[int, int]]) -> List[int]:
        """Находит самый длинный путь в графе, используя только доступные рёбра"""
        if not available_edges:
            return []

        # Начинаем с произвольного ребра
        start_edge = random.choice(list(available_edges))
        path = list(start_edge)

        while True:
            # Пытаемся расширить путь с обоих концов
            extended = False

            # Пробуем расширить в начало
            current = path[0]
            for neighbor in G.neighbors(current):
                edge = (neighbor, current) if (neighbor, current) in available_edges else (current, neighbor)
                if edge in available_edges and neighbor not in path:
                    path.insert(0, neighbor)
                    extended = True
                    break

            # Пробуем расширить в конец
            current = path[-1]
            for neighbor in G.neighbors(current):
                edge = (current, neighbor) if (current, neighbor) in available_edges else (neighbor, current)
                if edge in available_edges and neighbor not in path:
                    path.append(neighbor)
                    extended = True
                    break

            if not extended:
                break

        return path

    def generate_dataset(self):
        """Генерирует датасет с покрытиями для всех графов"""
        for G in self.graphs:
            chain_cover = self.find_chain_cover(G)
            optimal_cover = self.find_optimal_cover(G)
            self.chain_covers.append(chain_cover)
            self.optimal_covers.append(optimal_cover)

    def analyze_covers(self):
        """Анализирует отклонение решений от оптимальных"""
        chain_lengths = []
        optimal_lengths = []

        for chain_cover, optimal_cover in zip(self.chain_covers, self.optimal_covers):
            chain_lengths.append(len(chain_cover))
            optimal_lengths.append(len(optimal_cover))

        chain_mean = np.mean(chain_lengths)
        chain_std = np.std(chain_lengths)
        optimal_mean = np.mean(optimal_lengths)
        optimal_std = np.std(optimal_lengths)

        return {
            'chain_mean': chain_mean,
            'chain_std': chain_std,
            'optimal_mean': optimal_mean,
            'optimal_std': optimal_std
        }

# Создаём и анализируем датасет
dataset = GraphDataset(num_graphs=200)
dataset.generate_dataset()
stats = dataset.analyze_covers()

print("Статистический анализ:")
print(f"Среднее количество цепей в покрытии: {stats['chain_mean']:.2f} ± {stats['chain_std']:.2f}")
print(f"Среднее количество цепей в оптимальном покрытии: {stats['optimal_mean']:.2f} ± {stats['optimal_std']:.2f}")

# Визуализация одного из графов
def visualize_graph(G: nx.Graph, chains: List[List[int]] = None):
    pos = nx.spring_layout(G)  # Расположение узлов

    # Рисуем граф
    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)

    if chains:
        # Если переданы цепи, выделяем их разными цветами
        colors = plt.cm.get_cmap('tab10', len(chains))
        for i, chain in enumerate(chains):
            edges = [(chain[j], chain[j+1]) for j in range(len(chain)-1)]
            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=[colors(i)], width=2)

    plt.title("Визуализация графа")
    plt.show()

# Визуализируем первый граф и его цепное покрытие
sample_graph = dataset.graphs[0]
sample_cover = dataset.chain_covers[0]
visualize_graph(sample_graph, sample_cover)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T

class GraphCoverPredictor(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):
        super(GraphCoverPredictor, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)

        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x, edge_index, batch):
        # Graph convolution layers
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))

        # Global pooling
        x = global_mean_pool(x, batch)

        # MLP для финального предсказания
        return self.mlp(x)

class ChainCoverTrainer:
    def __init__(self, dataset):
        self.dataset = dataset
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = GraphCoverPredictor().to(self.device)

    def prepare_data(self):
        """Подготовка данных для обучения"""
        data_list = []

        for i, (G, optimal_cover) in enumerate(zip(self.dataset.graphs, self.dataset.optimal_covers)):
            # Преобразуем networkx граф в PyTorch Geometric формат
            pyg_graph = from_networkx(G)

            # Добавляем признаки вершин (используем degree как простой признак)
            degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
            pyg_graph.x = degrees

            # Целевое значение - количество цепей в оптимальном покрытии
            pyg_graph.y = torch.tensor([len(optimal_cover)], dtype=torch.float)

            data_list.append(pyg_graph)

        return data_list

    def train(self, num_epochs=10):
        """Обучение модели"""
        data_list = self.prepare_data()

        # Разделение на train/val
        train_size = int(0.8 * len(data_list))
        train_dataset = data_list[:train_size]
        val_dataset = data_list[train_size:]

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32)

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')

        # Списки для хранения истории обучения
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': []
        }

        for epoch in range(num_epochs):
            # Обучение
            self.model.train()
            total_loss = 0
            train_predictions = []
            train_targets = []

            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()

                out = self.model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                train_predictions.extend(out.cpu().detach().numpy())
                train_targets.extend(batch.y.cpu().numpy())

            # Вычисляем MAE для тренировочной выборки
            train_mae = np.mean([abs(p - t) for p, t in zip(train_predictions, train_targets)])

            # Валидация
            self.model.eval()
            val_loss = 0
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    out = self.model(batch.x, batch.edge_index, batch.batch)

                    val_loss += criterion(out, batch.y).item()
                    val_predictions.extend(out.cpu().numpy())
                    val_targets.extend(batch.y.cpu().numpy())

            val_mae = np.mean([abs(p - t) for p, t in zip(val_predictions, val_targets)])

            # Сохраняем метрики
            history['train_loss'].append(total_loss/len(train_loader))
            history['val_loss'].append(val_loss/len(val_loader))
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}:')
                print(f'Train Loss: {history["train_loss"][-1]:.4f}')
                print(f'Val Loss: {history["val_loss"][-1]:.4f}')
                print(f'Train MAE: {train_mae:.4f}')
                print(f'Val MAE: {val_mae:.4f}')

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pt')

        # Визуализация процесса обучения
        plt.figure(figsize=(12, 5))

        # График функции потерь
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (MSE)')
        plt.title('Learning Curves - Loss')
        plt.legend()
        plt.grid(True)

        # График MAE
        plt.subplot(1, 2, 2)
        plt.plot(history['train_mae'], label='Train MAE')
        plt.plot(history['val_mae'], label='Validation MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.title('Learning Curves - MAE')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

        return history

    def predict(self, G):
        """Предсказание количества цепей для нового графа"""
        self.model.eval()

        # Подготовка графа
        pyg_graph = from_networkx(G)
        degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
        pyg_graph.x = degrees

        # Создаем batch из одного графа
        loader = DataLoader([pyg_graph], batch_size=1)
        batch = next(iter(loader)).to(self.device)

        with torch.no_grad():
            pred = self.model(batch.x, batch.edge_index, batch.batch)

        return int(pred.item() + 0.5)  # округляем до ближайшего целого

# Создаем тренера и обучаем модель
trainer = ChainCoverTrainer(dataset)
trainer.train()

# Тестируем на новом графе
test_graph = nx.gnp_random_graph(120, 0.2)
while not nx.is_connected(test_graph):
    test_graph = nx.gnp_random_graph(120, 0.2)

predicted_chains = trainer.predict(test_graph)
actual_chains = len(dataset.find_optimal_cover(test_graph))

print("\nРезультаты тестирования:")
print(f"Предсказанное количество цепей: {predicted_chains}")
print(f"Фактическое количество цепей: {actual_chains}")
print(f"Отклонение: {abs(predicted_chains - actual_chains)}")

trainer.prepare_data()

def compare_results(dataset, trainer, num_test_graphs=50):
    """
    Сравнивает результаты ML модели со статистическими показателями
    """
    # Получаем статистики из п.3
    stats = dataset.analyze_covers()

    # Генерируем тестовые графы
    test_predictions = []
    test_actual = []

    for _ in range(num_test_graphs):
        # Создаем новый тестовый граф
        test_graph = nx.gnp_random_graph(120, 0.2)
        while not nx.is_connected(test_graph):
            test_graph = nx.gnp_random_graph(120, 0.2)

        # Получаем предсказание модели
        predicted = trainer.predict(test_graph)

        # Получаем реальное оптимальное покрытие
        actual = len(dataset.find_optimal_cover(test_graph))

        test_predictions.append(predicted)
        test_actual.append(actual)

    # Рассчитываем статистики для предсказаний
    ml_mean = np.mean(test_predictions)
    ml_std = np.std(test_predictions)

    # Рассчитываем ошибки
    mae = np.mean([abs(p - a) for p, a in zip(test_predictions, test_actual)])
    mse = np.mean([(p - a)**2 for p, a in zip(test_predictions, test_actual)])
    rmse = np.sqrt(mse)

    print("\nСравнительный анализ результатов:")
    print("\n1. Статистики из обучающего датасета (п.3):")
    print(f"Среднее количество цепей в оптимальном покрытии: {stats['optimal_mean']:.2f}")
    print(f"Стандартное отклонение: {stats['optimal_std']:.2f}")

    print("\n2. Статистики предсказаний ML модели:")
    print(f"Среднее количество предсказанных цепей: {ml_mean:.2f}")
    print(f"Стандартное отклонение предсказаний: {ml_std:.2f}")

    print("\n3. Метрики качества предсказаний:")
    print(f"Средняя абсолютная ошибка (MAE): {mae:.2f}")
    print(f"Среднеквадратичная ошибка (RMSE): {rmse:.2f}")

    print("\n4. Относительные отклонения:")
    print(f"Отклонение среднего значения: {abs(ml_mean - stats['optimal_mean'])/stats['optimal_mean']*100:.2f}%")
    print(f"Отклонение стандартного отклонения: {abs(ml_std - stats['optimal_std'])/stats['optimal_std']*100:.2f}%")

    # Визуализация результатов
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.hist(test_actual, bins=20, alpha=0.5, label='Реальные значения')
    plt.hist(test_predictions, bins=20, alpha=0.5, label='Предсказания')
    plt.axvline(stats['optimal_mean'], color='r', linestyle='--', label='Среднее из датасета')
    plt.axvline(ml_mean, color='g', linestyle='--', label='Среднее предсказаний')
    plt.xlabel('Количество цепей')
    plt.ylabel('Частота')
    plt.title('Распределение количества цепей')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.scatter(test_actual, test_predictions)
    plt.plot([min(test_actual), max(test_actual)],
             [min(test_actual), max(test_actual)],
             'r--', label='Идеальные предсказания')
    plt.xlabel('Реальные значения')
    plt.ylabel('Предсказания')
    plt.title('Предсказания vs Реальные значения')
    plt.legend()

    plt.tight_layout()
    plt.show()

    return {
        'training_stats': stats,
        'ml_stats': {
            'mean': ml_mean,
            'std': ml_std,
            'mae': mae,
            'rmse': rmse
        }
    }

# Запускаем сравнительный анализ
comparison_results = compare_results(dataset, trainer)

"""### 4 задание"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T
import random

def create_seed_graphs():
    """Создает набор графов-затравок."""
    seeds = {}

    # Затравка 1: треугольник
    G1 = nx.Graph()
    G1.add_edges_from([(0, 1), (1, 2), (2, 0)])
    seeds['triangle'] = G1

    # Затравка 2: звезда
    G2 = nx.Graph()
    G2.add_edges_from([(0, i) for i in range(1, 4)])
    seeds['star'] = G2

    # Затравка 3: путь
    G3 = nx.Graph()
    G3.add_edges_from([(0, 1), (1, 2)])
    seeds['path'] = G3

    # Затравка 4: квадрат
    G4 = nx.Graph()
    G4.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 0)])
    seeds['square'] = G4

    return seeds

# Определяем основные типы операций для связного графа
def add_vertex_with_edge(G, level, total_levels):
    """Добавить новую вершину с ребром к случайной существующей."""
    new_node = max(G.nodes) + 1 if len(G.nodes) > 0 else 0
    existing_node = random.choice(list(G.nodes))
    weight = 1 + level / total_levels  # Вес зависит от уровня
    G.add_node(new_node)
    G.add_edge(new_node, existing_node, weight=weight)

def add_edge(G, level, total_levels):
    """Добавить ребро между двумя случайными несмежными вершинами."""
    nodes = list(G.nodes)
    if len(nodes) > 1:
        u, v = random.sample(nodes, 2)
        if not G.has_edge(u, v):
            weight = 1 + level / total_levels  # Вес зависит от уровня
            G.add_edge(u, v, weight=weight)

def replace_vertex_with_seed(G, seeds, vertex, level, total_levels):
    """Заменить вершину затравкой, сохраняя связь с соседями."""
    if vertex not in G:
        return
    seed_key = random.choice(list(seeds.keys()))
    seed_graph = seeds[seed_key]

    # Сохраняем соседей текущей вершины
    neighbors = list(G.neighbors(vertex))
    G.remove_node(vertex)

    # Добавляем вершины и рёбра затравки
    mapping = {}
    for n in seed_graph.nodes:
        if n == 0:
            mapping[n] = vertex  # Сохраняем место удалённой вершины
            G.add_node(vertex)
        else:
            new_node = max(G.nodes) + 1
            G.add_node(new_node)
            mapping[n] = new_node

    for u, v in seed_graph.edges:
        weight = 1 + level / total_levels  # Вес зависит от уровня
        G.add_edge(mapping[u], mapping[v], weight=weight)

    # Восстанавливаем соседей к центральной вершине
    for neighbor in neighbors:
        weight = 1 + level / total_levels  # Вес зависит от уровня
        G.add_edge(vertex, neighbor, weight=weight)

# Основная функция эволюции графа
def evolve_graph(initial_seed, levels, num_operations):
    G = nx.Graph()
    G.add_nodes_from(initial_seed.nodes)
    G.add_edges_from(initial_seed.edges)

    seeds = create_seed_graphs()
    stages = []
    G_list = []  # Список для сохранения всех траекторий графа

    for level in range(levels):
        for _ in range(num_operations):
            operation = random.choice(
                ["add_vertex", "add_edge", "replace_vertex"]
            )
            if operation == "add_vertex":
                add_vertex_with_edge(G, level, levels)
            elif operation == "add_edge":
                add_edge(G, level, levels)
            elif operation == "replace_vertex" and len(G.nodes) > 0:
                vertex = random.choice(list(G.nodes))
                replace_vertex_with_seed(G, seeds, vertex, level, levels)

        # Сохраняем текущее состояние графа
        stages.append((list(G.nodes), list(G.edges(data=True))))
        G_list.append(G.copy())  # Сохраняем текущую копию графа

        # Проверка на минимальное количество вершин
        if len(G.nodes) >= 5000:
            break

    return G, stages, G_list

# Визуализация графа
def draw_graph(G):
    pos = nx.circular_layout(G)  # Раскладка графа
    edges = G.edges(data=True)

    # Отображаем рёбра с учётом их веса
#     weights = [edge[2].get('weight', 1) for edge in edges]
    nx.draw(G, pos, with_labels=False, node_size=10)
    plt.title("Эволюционный граф с весами рёбер")
    plt.show()

# Создание начального графа и эволюция
initial_seed = nx.complete_graph(5)  # Начальный граф - полный граф с 5 вершинами
levels = 300  # Увеличено число уровней
num_operations = 5  # Увеличено число операций на уровень
G, stages, G_list = evolve_graph(initial_seed, levels, num_operations)

# Подсчёт числа вершин
num_vertices = G.number_of_nodes()
print(f"Общее число вершин в графе: {num_vertices}")

# Визуализация финального графа
draw_graph(G)

# Проверка количества сохранённых траекторий
print(f"Сохранено траекторий графа: {len(G_list)}")

"""## Покрытие цепями"""

import networkx as nx
import random
import numpy as np
from typing import List, Set, Tuple
import matplotlib.pyplot as plt
from collections import defaultdict

class GraphDataset:
    def __init__(self, graphs):
        self.graphs = graphs  # Сохраняем список графов
        self.chain_covers = []
        self.optimal_covers = []
        self.num_graphs = len(graphs)

    def find_chain_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит покрытие графа непересекающимися цепями"""
        remaining_vertices = set(G.nodes())
        chains = []

        while remaining_vertices:
            # Начинаем новую цепь
            current_chain = []
            start_vertex = random.choice(list(remaining_vertices))
            current_chain.append(start_vertex)
            remaining_vertices.remove(start_vertex)

            while True:
                # Ищем следующую вершину для цепи
                current = current_chain[-1]
                neighbors = set(G.neighbors(current)) & remaining_vertices

                if not neighbors:
                    break

                next_vertex = random.choice(list(neighbors))
                current_chain.append(next_vertex)
                remaining_vertices.remove(next_vertex)

            chains.append(current_chain)

        return chains

    def find_optimal_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит оптимальное покрытие графа цепями (жадный алгоритм)"""
        remaining_edges = set(G.edges())
        chains = []

        while remaining_edges:
            # Находим самый длинный путь в оставшихся рёбрах
            current_chain = self._find_longest_path(G, remaining_edges)
            chains.append(current_chain)

            # Удаляем использованные рёбра
            for i in range(len(current_chain)-1):
                edge = (current_chain[i], current_chain[i+1])
                rev_edge = (current_chain[i+1], current_chain[i])
                if edge in remaining_edges:
                    remaining_edges.remove(edge)
                if rev_edge in remaining_edges:
                    remaining_edges.remove(rev_edge)

        # Добавляем изолированные вершины как отдельные цепи
        isolated_vertices = [v for v in G.nodes() if G.degree(v) == 0]
        for vertex in isolated_vertices:
            chains.append([vertex])

        return chains

    def _find_longest_path(self, G: nx.Graph, available_edges: Set[Tuple[int, int]]) -> List[int]:
        """Находит самый длинный путь в графе, используя только доступные рёбра"""
        if not available_edges:
            return []

        # Начинаем с произвольного ребра
        start_edge = random.choice(list(available_edges))
        path = list(start_edge)

        while True:
            # Пытаемся расширить путь с обоих концов
            extended = False

            # Пробуем расширить в начало
            current = path[0]
            for neighbor in G.neighbors(current):
                edge = (neighbor, current) if (neighbor, current) in available_edges else (current, neighbor)
                if edge in available_edges and neighbor not in path:
                    path.insert(0, neighbor)
                    extended = True
                    break

            # Пробуем расширить в конец
            current = path[-1]
            for neighbor in G.neighbors(current):
                edge = (current, neighbor) if (current, neighbor) in available_edges else (neighbor, current)
                if edge in available_edges and neighbor not in path:
                    path.append(neighbor)
                    extended = True
                    break

            if not extended:
                break

        return path

    def generate_dataset(self):
        """Генерирует датасет с покрытиями для всех графов"""
        self.chain_covers = []
        self.optimal_covers = []
        for G in self.graphs:
            chain_cover = self.find_chain_cover(G)
            optimal_cover = self.find_optimal_cover(G)
            self.chain_covers.append(chain_cover)
            self.optimal_covers.append(optimal_cover)

    def analyze_covers(self):
        """Анализирует отклонение решений от оптимальных"""
        if not self.chain_covers or not self.optimal_covers:
            raise ValueError("Необходимо сначала сгенерировать датасет")

        chain_lengths = []
        optimal_lengths = []

        for chain_cover, optimal_cover in zip(self.chain_covers, self.optimal_covers):
            chain_lengths.append(len(chain_cover))
            optimal_lengths.append(len(optimal_cover))

        chain_mean = np.mean(chain_lengths)
        chain_std = np.std(chain_lengths)
        optimal_mean = np.mean(optimal_lengths)
        optimal_std = np.std(optimal_lengths)

        return {
            'chain_mean': chain_mean,
            'chain_std': chain_std,
            'optimal_mean': optimal_mean,
            'optimal_std': optimal_std
        }

# Создаём и анализируем датасет
dataset = GraphDataset(G_list)
dataset.generate_dataset()
stats = dataset.analyze_covers()

print("Статистический анализ:")
print(f"Среднее количество цепей в покрытии: {stats['chain_mean']:.2f} ± {stats['chain_std']:.2f}")
print(f"Среднее количество цепей в оптимальном покрытии: {stats['optimal_mean']:.2f} ± {stats['optimal_std']:.2f}")

# Визуализация одного из графов
def visualize_graph(G: nx.Graph, chains: List[List[int]] = None):
    pos = nx.spring_layout(G)  # Расположение узлов

    # Рисуем граф
    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)

    if chains:
        # Если переданы цепи, выделяем их разными цветами
        colors = plt.cm.get_cmap('tab10', len(chains))
        for i, chain in enumerate(chains):
            edges = [(chain[j], chain[j+1]) for j in range(len(chain)-1)]
            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=[colors(i)], width=2)

    plt.title("Визуализация графа")
    plt.show()

# Визуализируем первый граф и его цепное покрытие
sample_graph = dataset.graphs[0]
sample_cover = dataset.chain_covers[0]
visualize_graph(sample_graph, sample_cover)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

class GraphCoverPredictor(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):
        super(GraphCoverPredictor, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)

        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.mlp(x)

class ChainCoverTrainer:
    def __init__(self, dataset):
        self.dataset = dataset
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = GraphCoverPredictor().to(self.device)

    def prepare_data(self):
        """Подготовка данных для обучения"""
        data_list = []

        for i, (G, optimal_cover) in enumerate(zip(self.dataset.graphs, self.dataset.optimal_covers)):
            # Добавляем одинаковый атрибут для всех рёбер
            for (u, v) in G.edges():
                G[u][v]['weight'] = 1.0

            # Преобразуем networkx граф в PyTorch Geometric формат
            pyg_graph = from_networkx(G)

            # Добавляем признаки вершин
            degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
            pyg_graph.x = degrees

            # Целевое значение
            pyg_graph.y = torch.tensor([len(optimal_cover)], dtype=torch.float)

            data_list.append(pyg_graph)

        return data_list

    def train(self, num_epochs=10):
        """Обучение модели"""
        data_list = self.prepare_data()

        train_size = int(0.8 * len(data_list))
        train_dataset = data_list[:train_size]
        val_dataset = data_list[train_size:]

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32)

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')

        history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': []
        }

        for epoch in range(num_epochs):
            # Обучение
            self.model.train()
            total_loss = 0
            train_predictions = []
            train_targets = []

            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()

                out = self.model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                train_predictions.extend(out.cpu().detach().numpy())
                train_targets.extend(batch.y.cpu().numpy())

            train_mae = np.mean([abs(p - t) for p, t in zip(train_predictions, train_targets)])

            # Валидация
            self.model.eval()
            val_loss = 0
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    out = self.model(batch.x, batch.edge_index, batch.batch)

                    val_loss += criterion(out, batch.y).item()
                    val_predictions.extend(out.cpu().numpy())
                    val_targets.extend(batch.y.cpu().numpy())

            val_mae = np.mean([abs(p - t) for p, t in zip(val_predictions, val_targets)])

            history['train_loss'].append(total_loss/len(train_loader))
            history['val_loss'].append(val_loss/len(val_loader))
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}:')
                print(f'Train Loss: {history["train_loss"][-1]:.4f}')
                print(f'Val Loss: {history["val_loss"][-1]:.4f}')
                print(f'Train MAE: {train_mae:.4f}')
                print(f'Val MAE: {val_mae:.4f}')

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pt')

        self.plot_history(history)
        return history

    def plot_history(self, history):
        """Визуализация процесса обучения"""
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (MSE)')
        plt.title('Learning Curves - Loss')
        plt.legend()
        plt.grid(True)

        plt.subplot(1, 2, 2)
        plt.plot(history['train_mae'], label='Train MAE')
        plt.plot(history['val_mae'], label='Validation MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.title('Learning Curves - MAE')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

    def predict(self, G):
        """Предсказание количества цепей для нового графа"""
        self.model.eval()

        # Добавляем атрибуты рёбер
        for (u, v) in G.edges():
            G[u][v]['weight'] = 1.0

        pyg_graph = from_networkx(G)
        degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
        pyg_graph.x = degrees

        loader = DataLoader([pyg_graph], batch_size=1)
        batch = next(iter(loader)).to(self.device)

        with torch.no_grad():
            pred = self.model(batch.x, batch.edge_index, batch.batch)

        return int(pred.item() + 0.5)

# # Создаем тренера и обучаем модель
trainer = ChainCoverTrainer(dataset)
trainer.train(num_epochs=10)

# Тестируем на новом графе
initial_seed1 = nx.complete_graph(2)  # Начальный граф - полный граф с 5 вершинами
levels1 = 5  # Увеличено число уровней
num_operations1 = 1  # Увеличено число операций на уровень
test_graph, _, _ = evolve_graph(initial_seed1, levels1, num_operations1)


predicted_chains = trainer.predict(test_graph)
actual_chains = len(dataset.find_optimal_cover(test_graph))

print("\nРезультаты тестирования:")
print(f"Предсказанное количество цепей: {predicted_chains}")
print(f"Фактическое количество цепей: {actual_chains}")
print(f"Отклонение: {abs(predicted_chains - actual_chains)}")

"""## Покрытие циклами"""

class CycleCoverGraph:
    def __init__(self, graphs):
        self.graphs = graphs
        self.covers = []
        self.num_graphs = len(graphs)

    def find_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит покрытие графа непересекающимися циклами"""
        remaining_vertices = set(G.nodes())
        cycles = []

        while remaining_vertices:
            current_cycle = []
            start_vertex = random.choice(list(remaining_vertices))
            current_cycle.append(start_vertex)
            remaining_vertices.remove(start_vertex)

            cycle_found = False
            while len(remaining_vertices) > 0:
                current = current_cycle[-1]
                neighbors = set(G.neighbors(current)) & remaining_vertices

                # Проверяем возможность замкнуть цикл
                if len(current_cycle) > 2:
                    start_neighbors = set(G.neighbors(current_cycle[0]))
                    if current in start_neighbors:
                        cycle_found = True
                        break

                if not neighbors:
                    break

                next_vertex = random.choice(list(neighbors))
                current_cycle.append(next_vertex)
                remaining_vertices.remove(next_vertex)

            if cycle_found:
                cycles.append(current_cycle)
            else:
                if len(current_cycle) > 0:
                    cycles.append(current_cycle)

        return cycles

    def generate_dataset(self):
        """Генерирует датасет с покрытиями для всех графов"""
        self.covers = []
        for G in self.graphs:
            cover = self.find_cover(G)
            self.covers.append(cover)

    def analyze_covers(self):
        """Анализирует характеристики покрытий"""
        if not self.covers:
            raise ValueError("Необходимо сначала сгенерировать датасет")

        cover_lengths = [len(cover) for cover in self.covers]

        return {
            'mean': np.mean(cover_lengths),
            'std': np.std(cover_lengths)
        }

# Создаём датасет
cycle_dataset = CycleCoverGraph(G_list)
cycle_dataset.generate_dataset()
cycle_stats = cycle_dataset.analyze_covers()

# Визуализация одного из графов
def visualize_graph(G: nx.Graph, chains: List[List[int]] = None):
    pos = nx.spring_layout(G)  # Расположение узлов

    # Рисуем граф
    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)

    if chains:
        # Если переданы цепи, выделяем их разными цветами
        colors = plt.cm.get_cmap('tab10', len(chains))
        for i, chain in enumerate(chains):
            edges = [(chain[j], chain[j+1]) for j in range(len(chain)-1)]
            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=[colors(i)], width=2)

    plt.title("Визуализация графа")
    plt.show()

# Визуализируем первый граф и его покрытие
sample_graph1 = cycle_dataset.graphs[0]
sample_cover1 = cycle_dataset.find_cover(sample_graph1)  # Вызываем метод find_cover
visualize_graph(sample_graph1, sample_cover1)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

class GraphCoverPredictor(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):
        super(GraphCoverPredictor, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)

        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.mlp(x)

class ChainCoverTrainer:
    def __init__(self, dataset):
        self.dataset = dataset
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = GraphCoverPredictor().to(self.device)

    def prepare_data(self):
        """Подготовка данных для обучения"""
        data_list = []

        for i, (G, cover) in enumerate(zip(self.dataset.graphs, self.dataset.covers)):  # Заменяем optimal_covers на covers
            # Добавляем одинаковый атрибут для всех рёбер
            for (u, v) in G.edges():
                G[u][v]['weight'] = 1.0

            # Преобразуем networkx граф в PyTorch Geometric формат
            pyg_graph = from_networkx(G)

            # Добавляем признаки вершин
            degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
            pyg_graph.x = degrees

            # Целевое значение
            pyg_graph.y = torch.tensor([len(cover)], dtype=torch.float)

            data_list.append(pyg_graph)

        return data_list

    def train(self, num_epochs=10):
        """Обучение модели"""
        data_list = self.prepare_data()

        train_size = int(0.8 * len(data_list))
        train_dataset = data_list[:train_size]
        val_dataset = data_list[train_size:]

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32)

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')

        history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': []
        }

        for epoch in range(num_epochs):
            # Обучение
            self.model.train()
            total_loss = 0
            train_predictions = []
            train_targets = []

            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()

                out = self.model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                train_predictions.extend(out.cpu().detach().numpy())
                train_targets.extend(batch.y.cpu().numpy())

            train_mae = np.mean([abs(p - t) for p, t in zip(train_predictions, train_targets)])

            # Валидация
            self.model.eval()
            val_loss = 0
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    out = self.model(batch.x, batch.edge_index, batch.batch)

                    val_loss += criterion(out, batch.y).item()
                    val_predictions.extend(out.cpu().numpy())
                    val_targets.extend(batch.y.cpu().numpy())

            val_mae = np.mean([abs(p - t) for p, t in zip(val_predictions, val_targets)])

            history['train_loss'].append(total_loss/len(train_loader))
            history['val_loss'].append(val_loss/len(val_loader))
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}:')
                print(f'Train Loss: {history["train_loss"][-1]:.4f}')
                print(f'Val Loss: {history["val_loss"][-1]:.4f}')
                print(f'Train MAE: {train_mae:.4f}')
                print(f'Val MAE: {val_mae:.4f}')

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pt')

        self.plot_history(history)
        return history

    def plot_history(self, history):
        """Визуализация процесса обучения"""
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (MSE)')
        plt.title('Learning Curves - Loss')
        plt.legend()
        plt.grid(True)

        plt.subplot(1, 2, 2)
        plt.plot(history['train_mae'], label='Train MAE')
        plt.plot(history['val_mae'], label='Validation MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.title('Learning Curves - MAE')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

    def predict(self, G):
        """Предсказание количества цепей для нового графа"""
        self.model.eval()

        # Добавляем атрибуты рёбер
        for (u, v) in G.edges():
            G[u][v]['weight'] = 1.0

        pyg_graph = from_networkx(G)
        degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
        pyg_graph.x = degrees

        loader = DataLoader([pyg_graph], batch_size=1)
        batch = next(iter(loader)).to(self.device)

        with torch.no_grad():
            pred = self.model(batch.x, batch.edge_index, batch.batch)

        return int(pred.item() + 0.5)

# Создаем тренера и обучаем модель
trainer = ChainCoverTrainer(cycle_dataset)
trainer.train(num_epochs=12)

predicted_chains = trainer.predict(test_graph)
actual_chains = len(cycle_dataset.find_cover(test_graph))

print("\nРезультаты тестирования:")
print(f"Предсказанное количество цепей: {predicted_chains}")
print(f"Фактическое количество цепей: {actual_chains}")
print(f"Отклонение: {abs(predicted_chains - actual_chains)}")

"""## Покрытие звёздами"""

class StarCoverDataset:
    def __init__(self, graphs):
        self.graphs = graphs
        self.covers = []
        self.num_graphs = len(graphs)

    def find_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит покрытие графа непересекающимися звёздами"""
        remaining_vertices = set(G.nodes())
        stars = []

        while remaining_vertices:
            # Выбираем центр звезды с максимальной степенью
            degrees = [(v, len(set(G.neighbors(v)) & remaining_vertices))
                      for v in remaining_vertices]
            center = max(degrees, key=lambda x: x[1])[0]

            current_star = [center]
            remaining_vertices.remove(center)

            # Добавляем лучи звезды
            neighbors = set(G.neighbors(center)) & remaining_vertices
            for neighbor in neighbors:
                current_star.append(neighbor)
                remaining_vertices.remove(neighbor)

            if len(current_star) > 1:
                stars.append(current_star)
            else:
                stars.append([center])

        return stars

    def generate_dataset(self):
        """Генерирует датасет с покрытиями для всех графов"""
        self.covers = []
        for G in self.graphs:
            cover = self.find_cover(G)
            self.covers.append(cover)

    def analyze_covers(self):
        """Анализирует характеристики покрытий"""
        if not self.covers:
            raise ValueError("Необходимо сначала сгенерировать датасет")

        cover_lengths = [len(cover) for cover in self.covers]

        return {
            'mean': np.mean(cover_lengths),
            'std': np.std(cover_lengths)
        }

star_dataset = StarCoverDataset(G_list)
star_dataset.generate_dataset()

# Создаем тренера и обучаем модель
trainer = ChainCoverTrainer(star_dataset)
trainer.train(num_epochs=12)

predicted_chains = trainer.predict(test_graph)
actual_chains = len(star_dataset.find_cover(test_graph))

print("\nРезультаты тестирования:")
print(f"Предсказанное количество цепей: {predicted_chains}")
print(f"Фактическое количество цепей: {actual_chains}")
print(f"Отклонение: {abs(predicted_chains - actual_chains)}")

