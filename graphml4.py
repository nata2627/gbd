# -*- coding: utf-8 -*-
"""GraphML4_ГордееваНГ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urKh72wVmlsWvU2NgWkCxZ0TIpHbFDus
"""

# data and utils
!gdown 1xRrXq3thIP9t05j8S2jAB1Khct7B4hT3 --folder
!gdown 17yFU0nPhOEgA847jSu0wcu3RdJwLyYzN --folder
# Install
!pip install rdkit==2023.09.6 # Install RDKit
!pip install dgl==1.0.0 # Install DGL
# Libraries
import pickle
import sys; sys.path.insert(0, 'lib/')
from lib.utils import Molecule
from rdkit import Chem
import torch
import torch.nn as nn
import networkx as nx
import matplotlib.pyplot as plt
import random
from lib.utils import compute_ncut
import dgl
from torch.utils.data import DataLoader
from lib.utils import compute_LapEig

"""# Данные для заданий 1-8 (молекулы)"""

print('Loading data')
data_folder_pytorch = 'datasets/ZINC_pytorch/'
with open(data_folder_pytorch+"train_pytorch.pkl","rb") as f:
    dataset=pickle.load(f)

# Select one molecule
idx = 12
mol = dataset[idx]
print(mol.atom_type)
print(mol.atom_type_pe)
print(mol.bond_type)
print(mol.bag_of_atoms)
print(mol.logP_SA_cycle_normalized)
print(mol.smile)
Chem.MolFromSmiles(mol.smile)

"""# Разработка алгоритма извлечение признаков с использованием случайных блужданий

## Задание 1: Реализуйте класс, который генерирует траекторию случайного блуждания.

Инструкции:
- Выберите следующий узел в соответствии с вероятностью случайного блуждания (RW) `prob_j = sample(RW[i,:])`.
- Используйте выборку Бернулли с помощью функции `torch.distributions.Categorical(prob).sample()`.
"""

class sample_RW_path:
    def __init__(self, num_steps, RW):
        self.num_steps = num_steps # number of steps
        self.RW = RW # random walk matrix
        self.num_nodes = RW.size(0) # number of nodes

    def sample_walk(self, idx_start):
        idx = torch.tensor(idx_start).long() # starting index of the walk
        RWpath = [idx] # random walk path
        for _ in range(self.num_steps-1):
            # Get probability distribution for next node from current node
            prob = self.RW[idx]
            # Sample next node using categorical distribution
            idx = torch.distributions.Categorical(prob).sample()
            RWpath.append(idx) # append sampled node to the path
        RWpath = torch.stack(RWpath).flatten()
        return RWpath

"""## Задание 2: Используйте ранее определенную функцию для выборки траектории случайного блуждания

Инструкции:
1. Вычислите оператор случайного блуждания по формуле $RW = D^{-1}A$.
1. Выберите количество шагов случайного блуждания (RW)
1. Используйте `sample_RW_path()` для создания экземпляра класса случайного блуждания (RW).
1. Примените `sample_walk()` для извлечения траектории случайного блуждания (RW).
"""

A = (mol.bond_type>0).float() # Adjacency matrix
D = A.sum(dim=0) # Degree vector

# Step 1: # Compute the RW operator
# Compute the inverse degree matrix
Dinv = torch.diag(1.0 / D)
# Compute RW matrix
RW = torch.mm(Dinv, A)

# Step 2: choose the number of RW steps
num_RW_steps = 4

# Step 3: instantiate RW class
generator = sample_RW_path(num_RW_steps, RW)

# Step 4: sample RW path starting with index=7
walk = generator.sample_walk(7)

print('RW:',walk)

"""## Задание 3: Визуализируйте выбранную траекторию случайного блуждания"""

# Check visually RW path correctness
fig = plt.figure()
ax = fig.add_subplot(111)
A_nx = nx.from_numpy_array(A.numpy())
C = compute_ncut(A.long(), 4)
nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes
ax.title.set_text('Molecule visualization with networkx')
plt.show()

"""# Реализация модели DeepWalk

## Задание 4: Реализуйте класс сети DeepWalk и примените его к молекулярным графам

Инструкции:
1. Извлеките эмбединги $h_i$ для узла $i$. Можете использовать метод `.unsqueeze()`.
1. Извлеките эмбединги $h_j$ для узлов $j$ в случайном блуждании (RW). Можете использовать метод `.transpose()`.
1. Извлеките эмбединги $h_k$ для узлов $k$ в случайном блуждании (RW).
"""

class deepwalk_net(nn.Module):
    def __init__(self, num_nodes, hidden_dim, num_negative):
        super(deepwalk_net, self).__init__()
        print(num_nodes, hidden_dim)
        self.num_nodes = num_nodes
        self.num_negative = num_negative
        self.node_embedding = nn.Embedding(num_nodes, hidden_dim)

    def forward(self, walk):
        walk_list = walk.tolist()
        node_list = list(range(self.num_nodes))
        list_negative = torch.tensor(list(set(node_list) - set(walk_list)))
        loss = []
        for i in walk:
            # Step1: extract embedding hi of node i
            hi = self.node_embedding(i).unsqueeze(0)

            # Step 2: extract embedding hj of nodes j in RW
            j = torch.tensor(list(set(walk_list) - set([i.detach().item()])))
            hj = self.node_embedding(j).transpose(0,1)

            # Step 3: extract embedding hk of nodes k not in RW
            list_negative = list_negative[torch.randperm(list_negative.size(0))][:self.num_negative]
            hk = self.node_embedding(list_negative).transpose(0,1)

            loss_i = -(torch.log(torch.sigmoid(torch.mm(hi,hj))).sum() -
                      0.25*torch.log(torch.sigmoid(torch.mm(hi,hk))).sum())
            loss.append(loss_i)
        loss = torch.stack(loss).mean()
        return loss

"""## Задание 5: Создание сети DeepWalk

Инструкции:
- Выберите количество отрицательных сэмплов и используйте функцию `deepwalk()` для создания экземпляра сети.
- Оцените и сравните производительность сетей DeepWalk с использованием разного количества отрицательных сэмплов.
"""

num_nodes = A.size(0)
# select num_negative = num_RW_steps/4
net = deepwalk_net(num_nodes, hidden_dim=2, num_negative=num_RW_steps//4)

print(net)

"""## Задание 6: Обучите сеть DeepWalk"""

# Train the network
optimizer = torch.optim.Adam( net.parameters() , lr=0.001 )
for iter in range(300):
    loss_epoch = 0.0
    for idx in torch.randperm(num_nodes).tolist(): # shuffle ordering of nodes
        walk = generator.sample_walk(idx)
        loss = net(walk)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        loss_epoch += loss.detach()
        with torch.no_grad():
            # centering the embedding coordinates
            # helps optimization by reducing one degree of freedom
            net.node_embedding.weight.sub_(net.node_embedding.weight.mean(dim=0))
    # plot the loss value
    if not iter%10:
        print(iter,loss_epoch/num_nodes)

"""## Задание 7: Визуализируйте эмбединги узлов"""

# Visualize the 2D coordinates of the node embeddings
x = net.node_embedding.weight.detach()
print(x,x.size())

# plot 2D coordinates
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(x[:,0], x[:,1])
idx = list(range(num_nodes))
ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')
for i, txt in enumerate(idx):
    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords="offset points", xytext=(1,5))
ax.title.set_text('2D embdding of nodes')
plt.show()

"""## Задание 8: Сравните визуализацию эмбедингов DeepWalk (из задания 7) с визуализацией NetworkX

"""

# Compare with graph edges
fig = plt.figure()
ax = fig.add_subplot(111)
nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes
ax.title.set_text('Molecular graph')
plt.show()

"""# Построение графа с использованием библиотеки Deep Graph Library (DGL)

## Задание 9: Создайте циклический граф из 7 узлов и конвертируйте его в двунаправленный граф

Инструкции:
- Используйте `dgl.graph(data, num_nodes=None)` для создания графа в DGL.  
- Граф инициализируется двумя списками: один содержит индексы узлов источника, а другой - соответствующие индексы узлов назначения. Эти списки определяют все ребра графа.
- Чтобы преобразовать направленный граф в неориентированный (двунаправленный), можете использовать `dgl.to_bidirected(graph)`.
"""

list_src = []
list_dst = []
num_nodes = 7
for i in range(num_nodes):
    list_src.append(i)
    list_dst.append((i+1)%num_nodes)
print([list_src,list_dst])

# create DGL graph
orig_graph_dgl = dgl.graph((list_src, list_dst), num_nodes=num_nodes)

# convert the graph to a symmetrize/undirected graph
graph_dgl = dgl.to_bidirected(orig_graph_dgl)

print(graph_dgl)

"""## Задание 10: Добавьте признаки узлов и ребер к ранее определенному графу DGL

Инструкции:
- Пусть `g = dgl.graph(data)` будет графом DGL.
- Чтобы добавить признаки узлов (например, "in-degree"(степень вхождения)), используйте `g.ndata['feat_name']`, где `'feat_name'` - это имя признака, который хотите назначить.
- Чтобы добавить признаки ребер, используйте `g.edata['feat_name']`, где `'feat_name'` соответствует имени признака для ребер.
- Чтобы вычислить степени вхождения для узлов, используйте `g.in_degrees().view(-1, 1).float()`. Это вернет тензор степеней вхождения для каждого узла.
Чтобы сгенерировать случайные признаки ребер, используйте `torch.rand(g.num_edges(), 100)`, который создаст случайные признаки с размерностью 100 для каждого ребра.
- Число ребер можно получить с помощью `g.num_edges()`.
"""

# Add node in-degrees as node features
graph_dgl.ndata['feat'] = graph_dgl.in_degrees().view(-1, 1).float()

# Add random edge features
graph_dgl.edata['feat'] = torch.rand(graph_dgl.num_edges(), 100)

print(graph_dgl)

"""# Вычисление функции передачи сообщений с помощью DGL

## Задание 11. Реализуйте шаг 1 функции передачи сообщений с помощью DGL
(Определите функцию сообщения для передачи признаков узлов и ребер вдоль ребер, т.е. от `src/j` к `dst/i`.

Инструкции:
- Чтобы получить доступ к признакам узлов назначения `dst`, используйте `edges.dst['feat_name']`, где `'feat_name'` - это имя признака для узлов назначения.
- Чтобы получить доступ к признакам узлов источника `src`, используйте `edges.src['feat_name']`, где `'feat_name'` - это имя признака для узлов источника.
- Чтобы получить доступ к признакам ребер, используйте `edges.data['feat_name']`, где `'feat_name'` - это имя признака для самих ребер.
"""

def message_func(edges):
    # Get destination node features
    hi = edges.dst['feat']

    # Get source node features
    hj = edges.src['feat']

    # Get edge features
    eji = edges.data['feat']

    # Update edge features
    edges.data['feat'] = eji

    print('hi', hi.size())
    print('hj', hj.size())
    print('eji', eji.size())

    return {'hj': hj, 'eji': eji}

"""## Задание 12: Реализуйте шаг 2 функции передачи сообщений с помощью DGL
(Определите функции уменьшения для вычисления суммы всех сообщений `{hj, eji}`, отправленных узлам `dst/i` на шаге 1.)

Инструкции:
- Чтобы получить доступ к признакам узла для каждого узла назначения `dst/i`, используйте `nodes.data['feat_name']`, где `'feat_name'` - это имя признака.
- Чтобы получить входящие сообщения для признака `hj`, используйте `nodes.mailbox['hj']`, который предоставляет входящие сообщения для каждого узла.
- Чтобы получить входящие признаки ребра `eji`, используйте `nodes.mailbox['eji']`.
- Обновите признак узла для каждого узла `i` в соответствии с уравнением:
$$h_i \leftarrow h_i + \sum_{j \in N(i)}e_{ji} \cdot h_j,$$
  где $N(i)$ множество соседей для узла `i`.

"""

# Step 2 of message-passing with DGL:
# Reduce function collects all messages={hj, eji} sent to node dst/i with Step 1
def reduce_func(nodes):
    # Get node features for destination nodes
    hi = nodes.data['feat']

    # Get all incoming node features
    hj = nodes.mailbox['hj']

    # Get all incoming edge features
    eji = nodes.mailbox['eji']

    # Update node features
    h = hi + torch.sum(eji * hj, dim=1)

    print('hi', hi.size())
    print('hj', hj.size())
    print('eji', eji.size())
    print('h', h.size())

    return {'h': h}

# Apply message passing
graph_dgl.update_all(message_func, reduce_func)

"""## Задание 13: Создайте батч из (двух) DGL графов
(Определите функцию `collate` для подготовки батчей графов, меток и других характеристик графов (если они есть)).

Инструкции:
- Пусть `graphs` - это набор графов DGL.
- Используйте `dgl.batch(graphs)`, чтобы объединить отдельные графы в один пакет.
"""

# generate two DGL graphs
list_src = torch.randperm(8).tolist()
list_dst = torch.randperm(8).tolist()
graph_dgl = dgl.graph((list_src, list_dst))
graph_dgl = dgl.to_bidirected(graph_dgl)
graph_dgl.ndata['feat'] = graph_dgl.in_degrees().view(-1, 1).float() # node in-degree
graph_dgl.edata['feat'] = torch.rand(graph_dgl.num_edges(), 100) # random features
g1 = graph_dgl
label1 = torch.tensor(0).long()
print('g1, label1', g1, label1)
list_src = torch.randperm(16).tolist()
list_dst = torch.randperm(16).tolist()
graph_dgl = dgl.graph((list_src, list_dst))
graph_dgl = dgl.to_bidirected(graph_dgl)
graph_dgl.ndata['feat'] = graph_dgl.in_degrees().view(-1, 1).float() # node in-degree
graph_dgl.edata['feat'] = torch.rand(graph_dgl.num_edges(), 100) # random features
g2 = graph_dgl
label2 = torch.tensor(1).long()
print('g2, label2', g2, label2)
trainset = [ [g1, label1] , [g2, label2] ]
print('trainset', trainset)

# define collate function prepares a batch of graphs, labels and other graph features (if needed)
def collate(samples):
    # Input sample is a list of pairs (graph, label)
    graphs, labels = map(list, zip(*samples))

    batch_graphs = dgl.batch(graphs)
    batch_labels = torch.stack(labels)

    return batch_graphs, batch_labels

# Generate a batch of graphs
batch_size = 2
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)
batch_graphs, batch_labels = list(train_loader)[0]
print('batch_graphs', batch_graphs)
print('batch_labels', batch_labels)
batch_x = batch_graphs.ndata['feat']
print('batch_x:',batch_x.size())
batch_e = batch_graphs.edata['feat']
print('batch_e:',batch_e.size())

"""# Преобразования между графами DGL, NetworkX, (плотными и разреженными) и PyTorch.

## Задание 14: Конвертируйте DGL граф в граф NetworkX

Инструкции:
- Пусть `g` - это граф DGL.
- Используйте `g.to_networkx()` для преобразования `g` в граф NetworkX.
"""

# build dgl graph
list_src = []
list_dst = []
num_nodes = 7
for i in range(num_nodes):
    list_src.append(i)
    list_dst.append((i+1)%num_nodes)
graph_dgl = dgl.graph((list_src, list_dst))
graph_dgl = dgl.to_bidirected(graph_dgl)

# dgl => networkx
graph_nx = graph_dgl.to_networkx()

"""## Задание 15: Визуализируйте граф с помощью NetworkX

Инструкции:
- Пусть `g` - это граф NetworkX,
- Используйте `networkx.draw(g)` для визуализации графа с помощью NetworkX.
  
"""

fig = plt.figure()
ax = fig.add_subplot(111)

nx.draw(graph_nx, with_labels=True, node_color='lightblue',
        node_size=500, arrowsize=20)

ax.title.set_text('Graph visualization with networkx')
plt.show()

"""Задание 16: Визуализируйте граф с помощью собственных векторов (т.е. отобразите граф в k-мерное евклидово пространство)

Инструкции:
- Пусть `g` - это DGL-граф.
- Используйте `compute_LapEig()` для получения первых двух собственных векторов DGL-графа.
- Визуализируйте граф с полученными двумя собственными векторами.
  
"""

fig = plt.figure()
ax = fig.add_subplot(111)
def compute_LapEig(graph_dgl, k=2):
    # Get adjacency matrix in sparse format
    A = graph_dgl.adj().to_dense().numpy()
    # Compute normalized laplacian
    from scipy import sparse
    from scipy.sparse.linalg import eigsh
    L = sparse.csgraph.laplacian(A, normed=True)
    # Compute eigenvalues and eigenvectors
    eigenval, eigenvec = eigsh(L, k=k, which='SM')
    return torch.from_numpy(eigenvec)

x = compute_LapEig(graph_dgl, k=2)
ax.scatter(x[:,0], x[:,1])
idx = list(range(graph_dgl.number_of_nodes()))
for i, txt in enumerate(idx):
    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords="offset points", xytext=(1,5))
ax.title.set_text('2D embedding of nodes')
plt.show()

"""## Задание 16: Конвертируйте DGL-граф в тензор PyTorch

Инструкции:
- Пусть `g` - это DGL-граф.
- Используйте `g.adj().to_dense()` для преобразования `g` в тензор PyTorch.

"""

graph_pytorch = graph_dgl.adj().to_dense()

print(graph_pytorch)

"""## Задание 17: Запустите код для вывода примеров всех конвертаций

- [NetworkX](https://networkx.org/documentation/stable/reference/convert.html)
- [Sparse PyTorch](https://pytorch.org/docs/stable/sparse.html)
- [DGL](https://docs.dgl.ai/api/python/dgl.sparse_v0.html#sparse-matrix-class)
"""

# dgl
print('# dgl\n',graph_dgl, type(graph_dgl)); print('\n')

# dgl => networkx
graph_nx = graph_dgl.to_networkx()
print('# dgl => networkx\n',graph_nx, type(graph_nx)); print('\n')

# networkx => dgl
graph_dgl = dgl.from_networkx(graph_nx)
print('# networkx => dgl\n',graph_dgl, type(graph_dgl))
print('Note: Node and edge features must be re-generated!'); print('\n')

# networkx => (coo) sparse pytorch
# Note: edges in sparse pytorch are structured as [ list_src, list_dst ]
src=[]; dst=[]; [(src.append(edge[0]), dst.append(edge[1])) for edge in nx.to_edgelist(graph_nx)] # extract list of edges
graph_sptorch = torch.sparse_coo_tensor([src, dst], [1]*len(src), size=(num_nodes,num_nodes))
print('# networkx => (coo) sparse pytorch\n',graph_sptorch, type(graph_sptorch)); print('\n')

# (coo) sparse pytorch => networkx
# Note: edges in networkx are structure as [ list(src,dst) ]
indices = graph_sptorch.coalesce().indices()
values = graph_sptorch.coalesce().values()
src = indices[0].tolist(); dst = indices[1].tolist(); list_edges = [edge for edge in zip(src,dst)]
graph_nx = nx.Graph(list_edges)
print('# (coo) sparse pytorch => networkx\n', graph_nx, type(graph_nx)); print('\n')

# (coo) sparse pytorch => dense pytorch
graph_pytorch = graph_sptorch.to_dense()
print('# (coo) sparse pytorch => dense pytorch\n', graph_pytorch, type(graph_pytorch)); print('\n')

# dense pytorch => (coo) sparse pytorch
graph_sptorch = graph_pytorch.to_sparse()
print('# dense pytorch => (coo) sparse pytorch\n', graph_sptorch, type(graph_sptorch)); print('\n')

# dgl => (coo) sparse pytorch
graph_sptorch = graph_dgl.adj()
print('# dgl => (coo) sparse pytorch\n', graph_sptorch, type(graph_sptorch)); print('\n')

# dgl => (coo) sparse pytorch => dense pytorch
graph_pytorch = graph_dgl.adj().to_dense()
print('# dgl => dense pytorch\n', graph_pytorch, type(graph_pytorch)); print('\n')