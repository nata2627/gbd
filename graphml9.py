# -*- coding: utf-8 -*-
"""GraphML9_ГордееваНГ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XR09kRVEYh8EzhSpaUIbCFzwQ5BDfwA8

**Обычные графовые трансформеры (без позиционного кодирования)**
"""

# data and utils
!gdown 1FNzZdwgVq8gsUXVKBzjNRo1ZQPw_j24K --folder
!gdown 1vFWcbAxKtejCsNIcT_kqErGNYNNXKxZL --folder
!pip install dgl==1.0.0 # Install DGL
# Libraries
import dgl
from dgl.data import MiniGCDataset
import dgl.function as fn
import matplotlib.pyplot as plt
import networkx as nx
import torch
from torch.utils.data import DataLoader
import torch.nn as nn
import time
import sys; sys.path.insert(0, 'lib/')
from lib.utils import compute_ncut

"""## Набор данных"""

dataset = MiniGCDataset(8, 10, 20) # DGL artificial dataset

# visualise the 8 classes of graphs
for c in range(8):
    graph, label = dataset[c]
    #fig, ax = plt.subplots()
    fig, ax = plt.subplots(figsize=(4,4))
    nx.draw(graph.to_networkx(), ax=ax)
    ax.set_title('Class: {:d}'.format(label))
    plt.show()

"""## Подготовка обучающего, валидационного и тестового датасетов

### Задание 1: Добавьте признаки узла


**Инструкции**:

- Вычислите in-degree для каждого узла с помощью ``g.in_degrees()``.

- Переформируйте и преобразуйте in-degrees в тензор с плавающей точкой с формой `(number_of_nodes, 1)`, используя `.view(-1, 1).float()`.

- Присвойте графам характеристики узлов, сохраняя их в словаре `ndata` с ключом `'feat'`: `g.ndata['feat']`.
"""

def add_node_edge_features(dataset):
    for (graph, _) in dataset:
        # Calculate in-degrees for each node
        in_degrees = graph.in_degrees()
        # Reshape to (num_nodes, 1) and convert to float
        node_features = in_degrees.view(-1, 1).float()
        # Assign features to graph
        graph.ndata['feat'] = node_features

    return dataset

# Generate graph datasets
trainset = MiniGCDataset(350, 10, 20)
testset = MiniGCDataset(100, 10, 20)
valset = MiniGCDataset(100, 10, 20)
trainset = add_node_edge_features(trainset)
testset = add_node_edge_features(testset)
valset = add_node_edge_features(valset)
print(trainset[0])

"""### Задание 2: Определите функцию collate для подготовки батча графов DGL

**Инструкции:**

- Используйте `map()` и `zip()` для преобразования выборок в отдельные списки графов и меток.

- Используйте функцию DGL `dgl.batch()` для объединения списка отдельных графов в один батч.

- Преобразуйте список меток в тензор PyTorch с помощью функции `torch.tensor()`.
"""

def collate(samples):
    # Input sample is a list of pairs (graph, label)
    # Use zip to separate graphs and labels into two lists
    graphs, labels = map(list, zip(*samples))

    # Combine graphs into a batch
    batch_graphs = dgl.batch(graphs)

    # Convert labels list to a tensor
    batch_labels = torch.tensor(labels)

    return batch_graphs, batch_labels

# Generate a batch of graphs
batch_size = 10
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)
batch_graphs, batch_labels = list(train_loader)[0]
print(batch_graphs)
print(batch_labels)
batch_x = batch_graphs.ndata['feat']
print('batch_x:',batch_x.size())

"""## Разработка класса нейронной сети графового трансформера (с помощью DGL)

Выражения для обновления весов узла:
\begin{eqnarray*}
\bar{h}^{\ell} &=&  h^{\ell} + \textrm{gMHA} (\textrm{LN}(h^{\ell})) \in \mathbb{R}^{N\times d}\\
h^{\ell+1} &=& \bar{h}^{\ell} + \textrm{MLP} (\textrm{LN}(\bar{h}^{\ell})) \in \mathbb{R}^{N\times d}\\
&&\textrm{with } \textrm{gMHA}(h)=\textrm{Concat}_{k=1}^H \left( \textrm{gHA}(h_k) \right) W_O \in \mathbb{R}^{N\times d},\ h_k\in \mathbb{R}^{N\times d'=d/H}, W_O\in \mathbb{R}^{d\times d} \\
&&\quad\quad\ \textrm{gHA}(h)=\textrm{Softmax}\left( A_G \odot \frac{QK^T}{\sqrt{d'}} \right) V \in \mathbb{R}^{N\times d'=d/H}, A_G\in \mathbb{R}^{N\times N} \textrm{ (матрица смежности графа)}\\
&&\quad\quad\ \textrm{gHA}(h)_i= \sum_{j\in \mathcal{N}_i} \underbrace{\frac{\exp(q_i^T k_j/\sqrt{d'})}{ \sum_{j'\in\mathcal{N}_i} \exp(q_i^T k_{j'}/\sqrt{d'}) }}_{\textrm{graph attention score}_{ij}} v_j\ \textrm{ (поэлементное уравнение)}\\
&&\quad\quad\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\in \mathbb{R}^{N\times d'=d/H}, W_Q, W_K, W_V\in \mathbb{R}^{d'\times d'}\\
h^{\ell=0} &=& \textrm{LL}(h_0) \in \mathbb{R}^{N\times d}\ \textrm{(признак входного узла)}\\
\end{eqnarray*}

### Класс полносвязанного слоя для классификации
"""

# class MLP layer for classification
class MLP_layer(nn.Module):

    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers
        super(MLP_layer, self).__init__()
        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]
        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))
        self.FC_layers = nn.ModuleList(list_FC_layers)
        self.L = L

    def forward(self, x):
        y = x
        for l in range(self.L):
            y = self.FC_layers[l](y)
            y = torch.relu(y)
        y = self.FC_layers[self.L](y)
        return y

"""### Задание 3: Реализуйте графовый Multi-Head Attention (MHA) с помощью DGL

**Инструкции:**

- *Шаг 1 передачи сообщений с помощью DGL.*\
Передаем характеристики узла и характеристики ребра по ребрам (src/j => dst/i) следующим образом:
  - *Шаг 1.1:* Вычислите $q_i^T * k_j$; можно использовать `edges.dst[]` для `i, edges.src[]` для `j`
  - *Шаг 1.2:* Вычислите $\textrm{exp}_{ij} = \exp( q_i^T * k_j / \sqrt{d'} )$, `size=(E,K,1)`.
  - *Шаг 1.3:* Получаем `V` по `edges.src['V'], size=(E,K,d')`.

- *Шаг 2 передачи сообщений с помощью DGL.*\
Определите функцию reduce, которая
  - *Шаг 2.1:* Использует `nodes.mailbox[]` для сбора всех сообщений `= {vj, eij}`, отправленных в узел dst/i с помощью *Шага 1*.
  - *Шаг 2.2:* Сумма/среднее по соседям графа `j` в `Ni`.

- *Шаг 3* присвоение значений.\
Присвойте графам значения `Q, K, V`, сохраняя их в словаре `ndata` с ключами `'Q', 'K', 'V':` `g.ndata['Q'], g.ndata['K'], g.ndata['V']` и перефоруйте их с помощью `.view(-1, num_heads, head_hidden_dim)`.
"""

class graph_MHA_layer(nn.Module):
    def __init__(self, hidden_dim, head_hidden_dim, num_heads):
        super().__init__()
        self.head_hidden_dim = head_hidden_dim  # d' = d/K
        self.num_heads = num_heads  # K
        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)
        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)
        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)

    def message_func(self, edges):
        # Step 1.1: Compute q_i^T * k_j
        # edges.dst['Q'] has shape (E, K, d'), edges.src['K'] has shape (E, K, d')
        # Result should have shape (E, K, 1)
        qk = torch.sum(edges.dst['Q'] * edges.src['K'], dim=2).unsqueeze(2)

        # Step 1.2: Compute exp_ij = exp(q_i^T * k_j / sqrt(d'))
        # Apply scaling factor and exponential
        scale = torch.sqrt(torch.tensor(self.head_hidden_dim, dtype=torch.float))
        expij = torch.exp(qk / scale)

        # Step 1.3: Get V from source nodes
        # edges.src['V'] has shape (E, K, d')
        vj = edges.src['V']

        return {'expij': expij, 'vj': vj}

    def reduce_func(self, nodes):
        # Step 2.1: Get all messages
        # nodes.mailbox['expij'] has shape (N, |Nj|, K, 1)
        expij = nodes.mailbox['expij']
        # nodes.mailbox['vj'] has shape (N, |Nj|, K, d')
        vj = nodes.mailbox['vj']

        # Step 2.2: Compute attention scores and weighted sum
        # Sum exponentials for normalization
        sum_expij = torch.sum(expij, dim=1, keepdim=True)  # (N, 1, K, 1)

        # Compute attention scores
        score = expij / (sum_expij + 1e-6)  # (N, |Nj|, K, 1)

        # Compute weighted sum of values
        # Expand scores to match value dimensions
        score = score.expand(-1, -1, -1, self.head_hidden_dim)  # (N, |Nj|, K, d')

        # Weighted sum over neighbors
        h = torch.sum(score * vj, dim=1)  # (N, K, d')

        return {'h': h}

    def forward(self, g, h):
        # Get Q, K, V projections
        Q = self.WQ(h)  # (N, K*d')
        K = self.WK(h)  # (N, K*d')
        V = self.WV(h)  # (N, K*d')

        # Reshape to separate heads
        N = h.size(0)
        Q = Q.view(N, self.num_heads, self.head_hidden_dim)  # (N, K, d')
        K = K.view(N, self.num_heads, self.head_hidden_dim)  # (N, K, d')
        V = V.view(N, self.num_heads, self.head_hidden_dim)  # (N, K, d')

        # Store Q, K, V in graph
        g.ndata['Q'] = Q
        g.ndata['K'] = K
        g.ndata['V'] = V

        # Apply message passing
        g.update_all(self.message_func, self.reduce_func)

        # Get result
        gMHA = g.ndata['h']  # (N, K, d')

        return gMHA

"""### Класс слоя графового трансформера"""

# class GraphTransformer layer
class GraphTransformer_layer(nn.Module):

    def __init__(self, hidden_dim, num_heads, dropout=0.0):
        super().__init__()
        self.hidden_dim = hidden_dim # hidden_dim = d
        self.num_heads = num_heads # number of heads = K
        self.dropout_mha = nn.Dropout(dropout) # dropout value
        self.dropout_mlp = nn.Dropout(dropout) # dropout value
        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer
        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP
        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP

    def forward(self, g, h):

        # Self-attention layer
        h_rc = h # size=(N,d), V=num_nodes, for residual connection
        h = self.layer_norm1(h) # layer normalization, size=(N, d)
        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)
        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)
        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)
        h_MHA = self.WO(h_MHA) # LL, size=(N, d)
        h = h_rc + h_MHA # residual connection, size=(N, d)

        # Fully-connected layer
        h_rc = h # for residual connection, size=(N, d)
        h = self.layer_norm2(h) # layer normalization, size=(N f, d)
        h_MLP = self.linear1(h) # LL, size=(N, d)
        h_MLP = torch.relu(h_MLP) # size=(N, d)
        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)
        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)
        h = h_rc + h_MLP # residual connection, size=(N, d)

        return h

"""### Задание 4: Объедините все ранее определенные слои, чтобы построить сеть Graph Transformer

**Инструкции:**

- *Слой эмбединга входных данных:* Инициализируйте линейный слой `nn.Linear()` для преобразования входных признаков в эмбединги узлов.

- *Слой графового трансформера:* Инициализируйте список модулей `nn.ModuleList()`, содержащий `L` экземпляры `GraphTransformer_layer()`.

- *MLP-слой:* Инициализируйте MLP-слой `MLP_layer()` для классификации.
"""

class GraphTransformer_layer(nn.Module):
    def __init__(self, hidden_dim, num_heads):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_hidden_dim = hidden_dim // num_heads

        # Multi-head attention
        self.gMHA = graph_MHA_layer(hidden_dim, self.head_hidden_dim, num_heads)
        self.WO = nn.Linear(hidden_dim, hidden_dim, bias=True)

        # Layer normalization and dropout
        self.layer_norm1 = nn.LayerNorm(hidden_dim)
        self.layer_norm2 = nn.LayerNorm(hidden_dim)
        self.dropout_mha = nn.Dropout(0.0)
        self.dropout_mlp = nn.Dropout(0.0)

        # MLP layers
        self.linear1 = nn.Linear(hidden_dim, hidden_dim, bias=True)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim, bias=True)

    def forward(self, g, h):
        # First sub-layer: Multi-head attention
        h_ln1 = self.layer_norm1(h)
        h_mha = self.gMHA(g, h_ln1)
        h_mha = h_mha.view(-1, self.hidden_dim)  # Reshape: (N, K, d') -> (N, d)
        h_mha = self.WO(h_mha)
        h_mha = self.dropout_mha(h_mha)
        h = h + h_mha  # Skip connection

        # Second sub-layer: MLP
        h_ln2 = self.layer_norm2(h)
        h_mlp = self.linear1(h_ln2)
        h_mlp = torch.relu(h_mlp)
        h_mlp = self.linear2(h_mlp)
        h_mlp = self.dropout_mlp(h_mlp)
        h = h + h_mlp  # Skip connection

        return h

class GraphTransformer_net(nn.Module):
    def __init__(self, net_parameters):
        super(GraphTransformer_net, self).__init__()
        input_dim = net_parameters['input_dim']
        hidden_dim = net_parameters['hidden_dim']
        output_dim = net_parameters['output_dim']
        num_heads = net_parameters['num_heads']
        L = net_parameters['L']

        # Input embedding layer
        self.embedding_h = nn.Linear(input_dim, hidden_dim, bias=True)

        # Graph Transformer layers
        self.GraphTransformer_layers = nn.ModuleList([
            GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L)
        ])

        # Output MLP layer for classification
        self.MLP_layer = MLP_layer(hidden_dim, output_dim, L=2)

    def forward(self, g, h):
        # Input embedding
        h = self.embedding_h(h)

        # Graph Transformer layers
        for layer in self.GraphTransformer_layers:
            h = layer(g, h)

        # Graph readout: mean of node features for each graph in the batch
        with g.local_scope():
            g.ndata['h'] = h
            # Calculate mean of node features for each graph
            h_graph = dgl.mean_nodes(g, 'h')

        # MLP classification layer
        y = self.MLP_layer(h_graph)

        return y

"""### Инициализация класса графовой нейронной сети"""

# Instantiate one network (testing)
net_parameters = {}
net_parameters['input_dim'] = 1
net_parameters['hidden_dim'] = 128
net_parameters['output_dim'] = 8 # nb of classes
net_parameters['num_heads'] = 8
net_parameters['L'] = 4
net = GraphTransformer_net(net_parameters)
print(net)
def display_num_param(net):
    nb_param = 0
    for param in net.parameters():
        nb_param += param.numel()
    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))
    return nb_param/1e6
_ = display_num_param(net)

batch_size = 10
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)
batch_graphs, batch_labels = list(train_loader)[0]
batch_x = batch_graphs.ndata['feat']
batch_labels = batch_labels
batch_scores = net(batch_graphs, batch_x)
print(batch_scores.size())

"""## Обучение модели"""

def accuracy(scores, targets):
    scores = scores.detach().argmax(dim=1)
    acc = (scores==targets).float().sum().item()
    return acc

def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):
    if train:
        net.train() # during training
    else:
        net.eval()  # during inference/test
    epoch_loss = 0
    epoch_acc = 0
    nb_data = 0
    gpu_mem = 0
    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):
        batch_x = batch_graphs.ndata['feat']
        batch_labels = batch_labels
        batch_scores = net.forward(batch_graphs, batch_x)
        loss = loss_fc(batch_scores, batch_labels)
        if train: # during training, run backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        epoch_loss += loss.detach().item()
        epoch_acc += accuracy(batch_scores, batch_labels)
        nb_data += batch_labels.size(0)
    epoch_loss /= (iter + 1)
    epoch_acc /= nb_data
    return epoch_loss, epoch_acc, optimizer

# dataset loaders
train_loader = DataLoader(trainset, batch_size=50, shuffle=True, collate_fn=collate)
test_loader = DataLoader(testset, batch_size=50, shuffle=False, collate_fn=collate)
val_loader = DataLoader(valset, batch_size=50, shuffle=False, drop_last=False, collate_fn=collate)

# Instantiate one network
net_parameters = {}
net_parameters['input_dim'] = 1
net_parameters['hidden_dim'] = 128
net_parameters['output_dim'] = 8 # nb of classes
net_parameters['num_heads'] = 8
net_parameters['L'] = 4
net = GraphTransformer_net(net_parameters)
_ = display_num_param(net)

# optimizer
loss_fc = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)

# training loop
for epoch in range(50):
    start = time.time()
    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, True, loss_fc, optimizer)
    with torch.no_grad():
        epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, False, loss_fc)
        epoch_val_loss, epoch_val_acc, _ = run_one_epoch(net, val_loader, False, loss_fc)
    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}, val_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss, epoch_val_loss))
    print('                       train_acc: {:.4f}, test_acc: {:.4f}, val_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc, epoch_val_acc))

"""## Задание 5: Интерпретация результатов

Замените в таблице `<ЗНАЧЕНИЕ>` на полученние результаты в ходе обучения модели:

| GNN    | train_acc | test_acc |
| -------- | ------- | ------- |
| GCN  | 0.7829   | 0.7900    |
| GIN | 0.0800     | 0.1000     |
| GAT    | 0.9229    | 0.9400    |
| Обычные графовые трансформеры | 0.9229 | 0.9400 |

"""

