# -*- coding: utf-8 -*-
"""Проект.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tlCrDAHYtP4Apt_2VyjkAg5hdJf5RKQG

# Этап I
"""

# !pip install ogb
# !pip install deap
# !pip install holoviews bokeh fa2
# !pip install fa2
# !pip install cython
# !pip install --upgrade pip setuptools

"""Импорт данных:"""

from ogb.nodeproppred import NodePropPredDataset
dataset = NodePropPredDataset(name='ogbn-arxiv')
graph, labels = dataset[0]

graph

import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict

def create_temporal_graphs(graph, node_year):
    """
    Создает последовательность графов по годам с учетом временных срезов
    """
    # Создаем базовый граф NetworkX
    G = nx.DiGraph()
    edges = graph['edge_index'].T

    # Добавляем все узлы и их годы
    for i, year in enumerate(node_year):
        G.add_node(i, year=int(year))

    # Добавляем все рёбра
    for source, target in edges:
        G.add_edge(source, target)

    # Создаем временные срезы графа
    dynamic_graphs = {}
    years = sorted(set(node_year))

    for year in years:
        # Выбираем только узлы, существующие к текущему году
        nodes = [n for n, d in G.nodes(data=True) if d['year'] <= year]
        subgraph = G.subgraph(nodes)
        dynamic_graphs[year] = subgraph

    return dynamic_graphs

node_years = graph['node_year'].flatten()
temporal_graphs = create_temporal_graphs(graph, node_years)

import networkx as nx
import matplotlib.pyplot as plt
from scipy.sparse import coo_matrix
import numpy as np

# Визуализация графов до 2010 года
for i, (year, subgraph) in enumerate(temporal_graphs.items()):
    if year > 2010:  # Ограничиваем годами до 2006 включительно
        break
    if i == 0 or i % 3 == 0:  # Первый граф и каждый 5-й
        plt.figure(figsize=(10, 7))
        nx.draw(subgraph, node_size=20, with_labels=False)
        plt.title(f"Graph for Year {year}")
        plt.show()

import networkx as nx
import matplotlib.pyplot as plt
from scipy.sparse import coo_matrix
import numpy as np

# Визуализация графов до 2006 года
for i, (year, subgraph) in enumerate(temporal_graphs.items()):
    if year > 2006:  # Ограничиваем годами до 2006 включительно
        break
    if i == 0 or i % 5 == 0:  # Первый граф и каждый 5-й
        plt.figure(figsize=(10, 7))
        nx.draw(subgraph, node_size=20, with_labels=False)
        plt.title(f"Graph for Year {year}")
        plt.show()

def export_for_neo4j(temporal_graphs, output_dir='neo4j_export'):
    """
    Экспортирует временные графы в формат CSV для импорта в Neo4j
    """
    import os
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Для каждого года создаем отдельные файлы узлов и рёбер
    for year, G in temporal_graphs.items():
        # Экспорт узлов
        nodes_file = f'{output_dir}/nodes_{year}.csv'
        with open(nodes_file, 'w') as f:
            f.write('nodeId:ID\n')  # Заголовок
            for node in G.nodes():
                f.write(f'{node}\n')

        # Экспорт рёбер
        edges_file = f'{output_dir}/edges_{year}.csv'
        with open(edges_file, 'w') as f:
            f.write(':START_ID,:END_ID,type\n')  # Заголовок
            for edge in G.edges():
                f.write(f'{edge[0]},{edge[1]},CITES\n')

# Экспортируем графы
export_for_neo4j(temporal_graphs)

def calculate_average_path_length(G):
    """
    Вычисляет среднюю длину пути с учетом несвязности графа
    """
    if nx.is_directed(G):
        components = nx.strongly_connected_components(G)
    else:
        components = nx.connected_components(G)

    # Вычисляем среднюю длину пути для каждой компоненты
    total_length = 0
    total_pairs = 0

    for component in components:
        if len(component) > 1:  # Пропускаем компоненты из одной вершины
            subgraph = G.subgraph(component)
            try:
                avg_length = nx.average_shortest_path_length(subgraph)
                n = len(component)
                # Количество возможных пар в компоненте
                pairs = n * (n - 1)
                total_length += avg_length * pairs
                total_pairs += pairs
            except:
                continue

    if total_pairs == 0:
        return float('inf')
    return total_length / total_pairs

import networkx as nx
import matplotlib.pyplot as plt
from scipy.sparse import coo_matrix
import numpy as np

# Визуализация графов до 2006 года
for i, (year, subgraph) in enumerate(dynamic_graphs.items()):
    if year > 2006:  # Ограничиваем годами до 2006 включительно
        break
    if i == 0 or i % 5 == 0:  # Первый граф и каждый 5-й
        plt.figure(figsize=(10, 7))
        nx.draw(subgraph, node_size=20, with_labels=False)
        plt.title(f"Graph for Year {year}")
        plt.show()

import time
from collections import defaultdict
from datetime import datetime

def print_progress(operation, year, time_taken):
    """
    Выводит прогресс выполнения операции в реальном времени
    """
    current_time = datetime.now().strftime("%H:%M:%S")
    print(f"[{current_time}] Год {year}, {operation}: {time_taken:.4f} сек")

def analyze_graph_metrics(G, year):
    """
    Вычисляет основные метрики графа с выводом времени в реальном времени
    """
    metrics = {}
    timing = {}

    # Базовые метрики
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Анализ года {year}:")

    start_time = time.time()
    metrics['nodes'] = G.number_of_nodes()
    metrics['edges'] = G.number_of_edges()
    metrics['density'] = nx.density(G)
    time_taken = time.time() - start_time
    timing['basic_metrics'] = time_taken
    print_progress("Базовые метрики", year, time_taken)

    # Анализ компонент связности
    start_time = time.time()
    if nx.is_directed(G):
        components = list(nx.strongly_connected_components(G))
    else:
        components = list(nx.connected_components(G))

    if components:
        largest_component = max(components, key=len)
        metrics['largest_component_size'] = len(largest_component)
        metrics['number_of_components'] = len(components)
    else:
        metrics['largest_component_size'] = 0
        metrics['number_of_components'] = 0
    time_taken = time.time() - start_time
    timing['components_analysis'] = time_taken
    print_progress("Анализ компонент", year, time_taken)

    # Кластеризация
    start_time = time.time()
    metrics['avg_clustering'] = nx.average_clustering(G.to_undirected())
    time_taken = time.time() - start_time
    timing['clustering'] = time_taken
    print_progress("Кластеризация", year, time_taken)

    # Средняя длина пути
    start_time = time.time()
    metrics['avg_shortest_path'] = calculate_average_path_length(G)
    time_taken = time.time() - start_time
    timing['path_length'] = time_taken
    print_progress("Средняя длина пути", year, time_taken)

    return metrics, timing

def analyze_temporal_graphs(temporal_graphs):
    """
    Анализирует изменение метрик графа во времени с выводом в реальном времени
    """
    temporal_metrics = {}
    temporal_timing = defaultdict(dict)
    total_start_time = time.time()

    total_years = len(temporal_graphs)
    current_year = 0

    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Начало анализа временного графа")
    print(f"Всего лет для анализа: {total_years}")

    for year, G in temporal_graphs.items():
        current_year += 1
        year_start_time = time.time()
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Прогресс: {current_year}/{total_years} ({(current_year/total_years*100):.1f}%)")

        metrics, timing = analyze_graph_metrics(G, year)
        temporal_metrics[year] = metrics
        temporal_timing[year] = timing

        year_time = time.time() - year_start_time
        temporal_timing[year]['total_year_time'] = year_time
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Год {year} завершен за {year_time:.4f} сек")

    total_time = time.time() - total_start_time
    temporal_timing['total_analysis_time'] = total_time
    print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Анализ завершен. Общее время: {total_time:.4f} сек")

    return temporal_metrics, temporal_timing

# Выполняем анализ
print(f"[{datetime.now().strftime('%H:%M:%S')}] Начало выполнения программы")
start_time = time.time()
metrics, timing = analyze_temporal_graphs(temporal_graphs)

# Выводим итоговую статистику
print("\nСредние значения времени выполнения операций:")
operations = ['basic_metrics', 'components_analysis', 'clustering', 'path_length', 'total_year_time']
for operation in operations:
    times = [timing[year][operation] for year in timing if isinstance(year, (int, float))]
    if times:
        avg_time = sum(times) / len(times)
        print(f"{operation}: {avg_time:.4f} сек")

print(f"\nОбщее время выполнения программы: {time.time() - start_time:.4f} сек")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

def visualize_temporal_metrics(metrics, output_dir='graph_analysis_plots'):
    """
    Создает визуализации для метрик временного графа
    Args:
        metrics: словарь с метриками графа
        output_dir: директория для сохранения графиков
    """
    import os
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # Подготовка данных
    years = sorted(y for y in metrics.keys())
    data = {
        'years': years,
        'nodes': [metrics[y]['nodes'] for y in years],
        'edges': [metrics[y]['edges'] for y in years],
        'density': [metrics[y]['density'] for y in years],
        'clustering': [metrics[y]['avg_clustering'] for y in years],
        'avg_path': [metrics[y]['avg_shortest_path'] for y in years],
        'components': [metrics[y]['number_of_components'] for y in years],
        'largest_component': [metrics[y]['largest_component_size'] for y in years]
    }

    # Настройка стиля
    sns.set_style("whitegrid")
    colors = sns.color_palette("husl", 8)

    # 1. Рост графа (узлы и рёбра)
    plt.figure(figsize=(12, 6))
    plt.plot(years, data['nodes'], marker='o', color=colors[0], label='Узлы')
    plt.plot(years, data['edges'], marker='s', color=colors[1], label='Рёбра')
    plt.xlabel('Год')
    plt.ylabel('Количество')
    plt.title('Рост графа во времени')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{output_dir}/growth.png')
    plt.close()

    # 2. Плотность и кластеризация
    plt.figure(figsize=(12, 6))
    plt.plot(years, data['density'], marker='o', color=colors[2], label='Плотность')
    plt.plot(years, data['clustering'], marker='s', color=colors[3], label='Коэффициент кластеризации')
    plt.xlabel('Год')
    plt.ylabel('Значение')
    plt.title('Изменение плотности и кластеризации')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{output_dir}/density_clustering.png')
    plt.close()

    # 3. Компоненты связности
    plt.figure(figsize=(12, 6))
    plt.plot(years, data['components'], marker='o', color=colors[4], label='Количество компонент')
    plt.plot(years, data['largest_component'], marker='s', color=colors[5],
             label='Размер наибольшей компоненты')
    plt.xlabel('Год')
    plt.ylabel('Количество')
    plt.title('Анализ компонент связности')
    plt.legend()
    plt.grid(True)
    plt.savefig(f'{output_dir}/components.png')
    plt.close()

    # 4. Тепловая карта корреляций
    plt.figure(figsize=(10, 8))
    metrics_df = pd.DataFrame(data)
    metrics_df = metrics_df.drop('years', axis=1)
    correlation_matrix = metrics_df.corr()
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Корреляция между метриками')
    plt.tight_layout()
    plt.savefig(f'{output_dir}/correlations.png')
    plt.close()

def print_summary_statistics(metrics):
    """
    Выводит сводную статистику по метрикам
    """
    print("\nСводная статистика:")
    years = sorted(y for y in metrics.keys())

    # Рост графа
    first_year = years[0]
    last_year = years[-1]
    growth_rate = (metrics[last_year]['nodes'] - metrics[first_year]['nodes']) / len(years)

    print(f"\nРост графа:")
    print(f"Начальное количество узлов ({first_year}): {metrics[first_year]['nodes']}")
    print(f"Конечное количество узлов ({last_year}): {metrics[last_year]['nodes']}")
    print(f"Средний годовой прирост узлов: {growth_rate:.2f}")

    print("\nСредние значения метрик:")
    for metric in ['density', 'avg_clustering', 'avg_shortest_path']:
        values = [metrics[y][metric] for y in years]
        values = np.array(values)
        # Исключаем бесконечные значения и NaN
        finite_values = values[np.isfinite(values)]
        if len(finite_values) > 0:
            mean = np.mean(finite_values)
            std = np.std(finite_values)
            print(f"{metric}: {mean:.4f} ± {std:.4f}")
        else:
            print(f"{metric}: Нет конечных значений")

# Пример использования:
if __name__ == "__main__":
    visualize_temporal_metrics(metrics)
    print_summary_statistics(metrics)

import json
import os
import pandas as pd
from datetime import datetime
import numpy as np

def save_metrics(metrics):
    """
    Сохраняет метрики графа в JSON и CSV форматах с конвертацией типов numpy

    Parameters:
    -----------
    metrics : dict
        Словарь с метриками графа по годам
    """
    # Конвертируем numpy.int64 в обычные int для ключей
    metrics_converted = {int(k): v for k, v in metrics.items()}

    # Создаём директорию для сохранения
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = 'graph_metrics_' + timestamp
    os.makedirs(save_dir, exist_ok=True)

    # Преобразуем numpy типы в обычные Python типы для JSON
    def convert_numpy(obj):
        if isinstance(obj, dict):
            return {k: convert_numpy(v) for k, v in obj.items()}
        elif isinstance(obj, (np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.float64, np.float32)):
            return float(obj)
        return obj

    metrics_converted = convert_numpy(metrics_converted)

    # Берем первые 30 элементов
    metrics_30 = dict(list(metrics_converted.items())[:30])

    # Сохраняем в JSON
    json_path = os.path.join(save_dir, 'metrics.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(metrics_30, f, indent=4, ensure_ascii=False)

    # Преобразуем в DataFrame и сохраняем в CSV
    df = pd.DataFrame.from_dict(metrics_30, orient='index')
    csv_path = os.path.join(save_dir, 'metrics.csv')
    df.to_csv(csv_path)

    print(f"Метрики сохранены в директории: {save_dir}")
    return save_dir

# Использование:
save_dir = save_metrics(metrics)

# Выводим результаты анализа
print("\nАнализ динамического графа по годам:")
for year in sorted(metrics.keys()):
    print(f"\nГод: {year}")
    for metric, value in metrics[year].items():
        if isinstance(value, float):
            print(f"{metric}: {value:.4f}")
        else:
            print(f"{metric}: {value}")

def visualize_temporal_graph(G, year, pos=None):
    """
    Визуализирует граф для конкретного года
    """
    plt.figure(figsize=(12, 8))

    if pos is None:
        pos = nx.spring_layout(G, k=1/np.sqrt(len(G)), iterations=50)

    # Рисуем узлы
    nx.draw_networkx_nodes(G, pos, node_size=20, node_color='lightblue')

    # Рисуем рёбра с минимальным пересечением
    nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.2, arrows=True)

    plt.title(f'Граф цитирования научных статей за {year} год k=5')
    plt.axis('off')
    return pos

# Визуализируем граф для нескольких лет
years_to_visualize = [2019]
pos = None
for year in years_to_visualize:
    G = temporal_graphs[year]
    # Берём подграф для лучшей визуализации
    subG = nx.k_core(G.to_undirected(), k=5).to_directed()
    pos = visualize_temporal_graph(subG, year, pos)
    plt.savefig(f'graph_{year}.png')
    plt.close()

"""# Этап II"""

temporal_graphs1 = dict(list(temporal_graphs.items())[:25])

import time
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import networkx as nx
import numpy as np
from community import community_louvain

class PerformanceTracker:
    def __init__(self):
        self.metrics = {
            'total_time': [],
            'betweenness_time': [],
            'clustering_time': [],
            'optimization_time': [],
            'nodes': [],
            'edges': [],
            'years': []
        }

    def add_measurement(self, year, G, times_dict):
        self.metrics['years'].append(year)
        self.metrics['nodes'].append(G.number_of_nodes())
        self.metrics['edges'].append(G.number_of_edges())
        for metric, value in times_dict.items():
            self.metrics[metric].append(value)

    def plot_performance(self, output_dir='performance_plots'):
        """Создает визуализации производительности"""
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # График времени выполнения по годам
        plt.figure(figsize=(12, 6))
        plt.plot(self.metrics['years'], self.metrics['total_time'],
                marker='o', label='Общее время')
        plt.plot(self.metrics['years'], self.metrics['betweenness_time'],
                marker='s', label='Вычисление центральности')
        plt.plot(self.metrics['years'], self.metrics['clustering_time'],
                marker='^', label='Кластеризация')
        plt.plot(self.metrics['years'], self.metrics['optimization_time'],
                marker='*', label='Оптимизация')
        plt.xlabel('Год')
        plt.ylabel('Время выполнения (секунды)')
        plt.title('Время выполнения алгоритма по годам')
        plt.legend()
        plt.grid(True)
        plt.savefig(f'{output_dir}/execution_time.png')
        plt.close()

        # График зависимости времени от размера графа
        plt.figure(figsize=(12, 6))
        plt.scatter(self.metrics['nodes'], self.metrics['total_time'],
                   alpha=0.6, label='Узлы')
        plt.xlabel('Количество узлов')
        plt.ylabel('Общее время выполнения (секунды)')
        plt.title('Зависимость времени выполнения от размера графа')
        plt.grid(True)
        plt.savefig(f'{output_dir}/size_vs_time.png')
        plt.close()

class InfluentialClusterFinder:
    def __init__(self, G, min_cluster_size=5, max_cluster_size=20):
        self.G = G
        self.min_cluster_size = min_cluster_size
        self.max_cluster_size = max_cluster_size
        self.performance_metrics = {}

    def calculate_betweenness_centrality(self):
        start_time = time.time()
        result = nx.betweenness_centrality(self.G)
        self.performance_metrics['betweenness_time'] = time.time() - start_time
        return result

    def calculate_cluster_density(self, nodes):
        """Вычисляет плотность связей в кластере"""
        if len(nodes) < 2:
            return 0
        subgraph = self.G.subgraph(nodes)
        return nx.density(subgraph)

    def calculate_average_distance(self, nodes):
        """Вычисляет среднее расстояние между узлами кластера"""
        if len(nodes) < 2:
            return float('inf')
        subgraph = self.G.subgraph(nodes)
        try:
            if nx.is_connected(subgraph.to_undirected()):
                return nx.average_shortest_path_length(subgraph)
            else:
                # Для несвязных графов возвращаем бесконечность
                return float('inf')
        except:
            return float('inf')

    def find_optimal_clusters(self, alpha=0.4, beta=0.3, gamma=0.3):
        total_start_time = time.time()

        # Проверяем, есть ли достаточно узлов для кластеризации
        if self.G.number_of_nodes() < self.min_cluster_size:
            self.performance_metrics['betweenness_time'] = 0
            self.performance_metrics['clustering_time'] = 0
            self.performance_metrics['optimization_time'] = 0
            self.performance_metrics['total_time'] = time.time() - total_start_time
            return []

        # Замеряем время вычисления центральности
        betweenness = self.calculate_betweenness_centrality()

        # Замеряем время кластеризации
        clustering_start_time = time.time()
        communities = community_louvain.best_partition(self.G.to_undirected())
        self.performance_metrics['clustering_time'] = time.time() - clustering_start_time

        # Замеряем время оптимизации
        optimization_start_time = time.time()
        optimal_clusters = []

        for community_id in set(communities.values()):
            community_nodes = [node for node, com_id in communities.items()
                             if com_id == community_id]

            if len(community_nodes) < self.min_cluster_size:
                continue

            density = self.calculate_cluster_density(community_nodes)
            avg_distance = self.calculate_average_distance(community_nodes)
            avg_betweenness = np.mean([betweenness[node] for node in community_nodes])

            score = (alpha * avg_betweenness +
                    beta * density +
                    gamma * (1 / (1 + avg_distance)))

            optimal_clusters.append({
                'nodes': community_nodes,
                'score': score,
                'metrics': {
                    'betweenness': avg_betweenness,
                    'density': density,
                    'avg_distance': avg_distance
                }
            })

        optimal_clusters.sort(key=lambda x: x['score'], reverse=True)
        self.performance_metrics['optimization_time'] = time.time() - optimization_start_time
        self.performance_metrics['total_time'] = time.time() - total_start_time

        return optimal_clusters

def analyze_temporal_clusters(temporal_graphs):
    """Анализирует изменение кластеров во времени с отслеживанием производительности"""
    temporal_clusters = {}
    performance_tracker = PerformanceTracker()

    for year, G in temporal_graphs.items():
        print(f"\nАнализ графа за {year} год...")
        print(f"Узлов: {G.number_of_nodes()}, Рёбер: {G.number_of_edges()}")

        finder = InfluentialClusterFinder(G)
        clusters = finder.find_optimal_clusters()

        # Сохраняем метрики производительности
        performance_tracker.add_measurement(year, G, finder.performance_metrics)

        temporal_clusters[year] = clusters

        print(f"Найдено кластеров: {len(clusters)}")
        print(f"Время выполнения: {finder.performance_metrics['total_time']:.2f} сек")

    # Создаем визуализации производительности
    performance_tracker.plot_performance()

    return temporal_clusters, performance_tracker

# Применяем алгоритм к временным графам
temporal_clusters, performance = analyze_temporal_clusters(temporal_graphs1)

import time
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import networkx as nx
import numpy as np
from community import community_louvain

class PerformanceTracker:
    def __init__(self):
        self.metrics = {
            'total_time': [],
            'betweenness_time': [],
            'clustering_time': [],
            'optimization_time': [],
            'nodes': [],
            'edges': [],
            'years': []
        }

    def add_measurement(self, year, G, times_dict):
        self.metrics['years'].append(year)
        self.metrics['nodes'].append(G.number_of_nodes())
        self.metrics['edges'].append(G.number_of_edges())
        for metric, value in times_dict.items():
            self.metrics[metric].append(value)

    def plot_performance(self, output_dir='performance_plots'):
        """Создает визуализации производительности"""
        import os
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # График времени выполнения по годам
        plt.figure(figsize=(12, 6))
        plt.plot(self.metrics['years'], self.metrics['total_time'],
                marker='o', label='Общее время')
        plt.plot(self.metrics['years'], self.metrics['betweenness_time'],
                marker='s', label='Вычисление центральности')
        plt.plot(self.metrics['years'], self.metrics['clustering_time'],
                marker='^', label='Кластеризация')
        plt.plot(self.metrics['years'], self.metrics['optimization_time'],
                marker='*', label='Оптимизация')
        plt.xlabel('Год')
        plt.ylabel('Время выполнения (секунды)')
        plt.title('Время выполнения алгоритма по годам')
        plt.legend()
        plt.grid(True)
        plt.savefig(f'{output_dir}/execution_time.png')
        plt.close()

        # График зависимости времени от размера графа
        plt.figure(figsize=(12, 6))
        plt.scatter(self.metrics['nodes'], self.metrics['total_time'],
                   alpha=0.6, label='Узлы')
        plt.xlabel('Количество узлов')
        plt.ylabel('Общее время выполнения (секунды)')
        plt.title('Зависимость времени выполнения от размера графа')
        plt.grid(True)
        plt.savefig(f'{output_dir}/size_vs_time.png')
        plt.close()

class InfluentialClusterFinder:
    def __init__(self, G, min_cluster_size=5, max_cluster_size=20):
        self.G = G
        self.min_cluster_size = min_cluster_size
        self.max_cluster_size = max_cluster_size
        self.performance_metrics = {}

    def calculate_betweenness_centrality(self):
        start_time = time.time()
        result = nx.betweenness_centrality(self.G)
        self.performance_metrics['betweenness_time'] = time.time() - start_time
        return result

    def calculate_cluster_density(self, nodes):
        """Вычисляет плотность связей в кластере"""
        if len(nodes) < 2:
            return 0
        subgraph = self.G.subgraph(nodes)
        return nx.density(subgraph)

    def calculate_average_distance(self, nodes):
        """Вычисляет среднее расстояние между узлами кластера"""
        if len(nodes) < 2:
            return float('inf')
        subgraph = self.G.subgraph(nodes)
        try:
            if nx.is_connected(subgraph.to_undirected()):
                return nx.average_shortest_path_length(subgraph)
            else:
                # Для несвязных графов возвращаем бесконечность
                return float('inf')
        except:
            return float('inf')

    def find_optimal_clusters(self, alpha=0.4, beta=0.3, gamma=0.3):
        total_start_time = time.time()

        # Проверяем, есть ли достаточно узлов для кластеризации
        if self.G.number_of_nodes() < self.min_cluster_size:
            self.performance_metrics['betweenness_time'] = 0
            self.performance_metrics['clustering_time'] = 0
            self.performance_metrics['optimization_time'] = 0
            self.performance_metrics['total_time'] = time.time() - total_start_time
            return []

        # Замеряем время вычисления центральности
        betweenness = self.calculate_betweenness_centrality()

        # Замеряем время кластеризации
        clustering_start_time = time.time()
        communities = community_louvain.best_partition(self.G.to_undirected())
        self.performance_metrics['clustering_time'] = time.time() - clustering_start_time

        # Замеряем время оптимизации
        optimization_start_time = time.time()
        optimal_clusters = []

        for community_id in set(communities.values()):
            community_nodes = [node for node, com_id in communities.items()
                             if com_id == community_id]

            if len(community_nodes) < self.min_cluster_size:
                continue

            density = self.calculate_cluster_density(community_nodes)
            avg_distance = self.calculate_average_distance(community_nodes)
            avg_betweenness = np.mean([betweenness[node] for node in community_nodes])

            score = (alpha * avg_betweenness +
                    beta * density +
                    gamma * (1 / (1 + avg_distance)))

            optimal_clusters.append({
                'nodes': community_nodes,
                'score': score,
                'metrics': {
                    'betweenness': avg_betweenness,
                    'density': density,
                    'avg_distance': avg_distance
                }
            })

        optimal_clusters.sort(key=lambda x: x['score'], reverse=True)
        self.performance_metrics['optimization_time'] = time.time() - optimization_start_time
        self.performance_metrics['total_time'] = time.time() - total_start_time

        return optimal_clusters

def analyze_temporal_clusters(temporal_graphs):
    """Анализирует изменение кластеров во времени с отслеживанием производительности"""
    temporal_clusters = {}
    performance_tracker = PerformanceTracker()

    for year, G in temporal_graphs.items():
        print(f"\nАнализ графа за {year} год...")
        print(f"Узлов: {G.number_of_nodes()}, Рёбер: {G.number_of_edges()}")

        finder = InfluentialClusterFinder(G)
        clusters = finder.find_optimal_clusters()

        # Сохраняем метрики производительности
        performance_tracker.add_measurement(year, G, finder.performance_metrics)

        temporal_clusters[year] = clusters

        print(f"Найдено кластеров: {len(clusters)}")
        print(f"Время выполнения: {finder.performance_metrics['total_time']:.2f} сек")

    # Создаем визуализации производительности
    performance_tracker.plot_performance()

    return temporal_clusters, performance_tracker

# Применяем алгоритм к временным графам
temporal_clusters, performance = analyze_temporal_clusters(temporal_graphs1)

import json
import csv
from datetime import datetime
import os

def save_temporal_results(temporal_clusters, performance_tracker, output_dir='clustering_results'):
    """
    Сохраняет результаты временного анализа кластеров и метрики производительности

    Parameters:
    -----------
    temporal_clusters : dict
        Словарь с результатами кластеризации по годам
    performance_tracker : PerformanceTracker
        Объект с метриками производительности
    output_dir : str
        Директория для сохранения результатов
    """
    # Создаем директорию если её нет
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # Сохраняем результаты кластеризации
    clusters_file = f'{output_dir}/clusters_{timestamp}.json'
    clusters_data = {}

    for year, clusters in temporal_clusters.items():
        clusters_data[str(year)] = [{
            'nodes': cluster['nodes'],
            'score': cluster['score'],
            'metrics': {
                'betweenness': float(cluster['metrics']['betweenness']),
                'density': float(cluster['metrics']['density']),
                'avg_distance': float(cluster['metrics']['avg_distance'])
            }
        } for cluster in clusters]

    with open(clusters_file, 'w', encoding='utf-8') as f:
        json.dump(clusters_data, f, indent=2, ensure_ascii=False)

    # Сохраняем метрики производительности
    performance_file = f'{output_dir}/performance_{timestamp}.csv'

    with open(performance_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # Записываем заголовки
        writer.writerow(['year', 'nodes', 'edges', 'total_time',
                        'betweenness_time', 'clustering_time', 'optimization_time'])

        # Записываем данные
        for i, year in enumerate(performance_tracker.metrics['years']):
            writer.writerow([
                year,
                performance_tracker.metrics['nodes'][i],
                performance_tracker.metrics['edges'][i],
                performance_tracker.metrics['total_time'][i],
                performance_tracker.metrics['betweenness_time'][i],
                performance_tracker.metrics['clustering_time'][i],
                performance_tracker.metrics['optimization_time'][i]
            ])

    print(f"Результаты кластеризации сохранены в: {clusters_file}")
    print(f"Метрики производительности сохранены в: {performance_file}")

# Использование:
save_temporal_results(temporal_clusters, performance)

def visualize_optimal_clusters(G, clusters, year, pos=None, num_clusters=10):
    """
    Визуализирует оптимальные кластеры в графе с случайными цветами

    Parameters:
    -----------
    G : networkx.Graph
        Граф для визуализации
    clusters : list
        Список кластеров с их метриками
    year : int
        Год для заголовка
    pos : dict, optional
        Словарь позиций узлов
    num_clusters : int, optional
        Количество кластеров для отображения
    """
    plt.figure(figsize=(20, 15))

    # Генерируем случайные цвета для кластеров
    import numpy as np

    def generate_random_color():
        # Генерируем яркие случайные цвета
        hue = np.random.rand()  # Случайный оттенок
        saturation = 0.6 + np.random.rand() * 0.4  # Высокая насыщенность (0.6-1.0)
        value = 0.6 + np.random.rand() * 0.4  # Высокая яркость (0.6-1.0)

        import colorsys
        rgb = colorsys.hsv_to_rgb(hue, saturation, value)
        return rgb

    # Получаем список всех узлов
    all_nodes = set(G.nodes())
    cluster_nodes = set()
    for cluster in clusters[:num_clusters]:
        cluster_nodes.update(cluster['nodes'])

    if pos is None or not all(node in pos for node in all_nodes):
        pos = nx.spring_layout(G, k=2/np.sqrt(len(G)), iterations=100)

    # Рисуем фоновые узлы
    background_nodes = all_nodes - cluster_nodes
    if background_nodes:
        nx.draw_networkx_nodes(G, pos,
                             nodelist=list(background_nodes),
                             node_size=50,
                             node_color='lightgray',
                             alpha=0.3)

    # Рисуем рёбра
    nx.draw_networkx_edges(G, pos,
                          edge_color='lightgray',
                          alpha=0.1,
                          width=0.5)

    # Генерируем список случайных цветов для всех кластеров
    random_colors = [generate_random_color() for _ in range(num_clusters)]

    # Рисуем кластеры
    for i, cluster in enumerate(clusters[:num_clusters]):
        nodes = [n for n in cluster['nodes'] if n in all_nodes]
        if nodes:
            color = random_colors[i]

            # Рисуем рёбра внутри кластера
            subgraph = G.subgraph(nodes)
            nx.draw_networkx_edges(G, pos,
                                 edgelist=list(subgraph.edges()),
                                 edge_color=[color],
                                 alpha=0.3,
                                 width=1.0)

            # Рисуем узлы кластера
            nx.draw_networkx_nodes(G, pos,
                                 nodelist=nodes,
                                 node_color=[color],
                                 node_size=200,
                                 label=f'Cluster {i+1} (score: {cluster["score"]:.3f})',
                                 alpha=0.8,
                                 edgecolors='white',
                                 linewidths=1.5)

    plt.title(f'Оптимальные научные кластеры за {year} год\n(показаны топ-{num_clusters} кластеров)',
              fontsize=16,
              pad=20)
    plt.legend(fontsize=10,
              loc='center left',
              bbox_to_anchor=(1, 0.5),
              ncol=1)
    plt.axis('off')
    plt.margins(0.2)

    return pos

# Использование:
years_to_visualize = [2008]
pos = None
for year in years_to_visualize:
    G = temporal_graphs[year]
    clusters = temporal_clusters[year]
    pos = visualize_optimal_clusters(G, clusters, year, pos, num_clusters=10)
    plt.savefig(f'clusters_{year}.png',
                dpi=300,
                bbox_inches='tight',
                pad_inches=0.5)
    plt.close()

def visualize_optimal_clusters(G, clusters, year, pos=None, num_clusters=10):
    """
    Визуализирует оптимальные кластеры в графе

    Parameters:
    -----------
    G : networkx.Graph
        Граф для визуализации
    clusters : list
        Список кластеров с их метриками
    year : int
        Год для заголовка
    pos : dict, optional
        Словарь позиций узлов
    num_clusters : int, optional
        Количество кластеров для отображения (по умолчанию 10)
    """
    plt.figure(figsize=(20, 15))

    # Получаем список всех узлов
    all_nodes = set(G.nodes())
    cluster_nodes = set()
    for cluster in clusters[:num_clusters]:
        cluster_nodes.update(cluster['nodes'])

    if pos is None or not all(node in pos for node in all_nodes):
        pos = nx.spring_layout(G, k=2/np.sqrt(len(G)), iterations=100)

    # Рисуем фоновые узлы
    background_nodes = all_nodes - cluster_nodes
    if background_nodes:
        nx.draw_networkx_nodes(G, pos,
                             nodelist=list(background_nodes),
                             node_size=50,
                             node_color='lightgray',
                             alpha=0.3)

    # Рисуем рёбра
    nx.draw_networkx_edges(G, pos,
                          edge_color='lightgray',
                          alpha=0.1,
                          width=0.5)

    # Создаем цветовую палитру для большего количества кластеров
    colors = [
        '#FF4444', '#4444FF', '#44FF44',  # красный, синий, зеленый
        '#FFB000', '#9F44FF', '#44FFFF',  # оранжевый, фиолетовый, голубой
        '#FF44B0', '#B0FF44', '#8B4513',  # розовый, лайм, коричневый
        '#FF8C00', '#483D8B', '#006400',  # темно-оранжевый, темно-синий, темно-зеленый
        '#8B008B', '#DAA520', '#00CED1'   # пурпурный, золотой, бирюзовый
    ][:num_clusters]

    # Выделяем кластеры разными цветами
    for i, cluster in enumerate(clusters[:num_clusters]):
        nodes = [n for n in cluster['nodes'] if n in all_nodes]
        if nodes:
            # Рисуем рёбра внутри кластера
            subgraph = G.subgraph(nodes)
            nx.draw_networkx_edges(G, pos,
                                 edgelist=list(subgraph.edges()),
                                 edge_color=colors[i],
                                 alpha=0.3,
                                 width=1.0)

            # Рисуем узлы кластера
            nx.draw_networkx_nodes(G, pos,
                                 nodelist=nodes,
                                 node_color=colors[i],
                                 node_size=200,
                                 label=f'Cluster {i+1} (score: {cluster["score"]:.3f})',
                                 alpha=0.8,
                                 edgecolors='white',
                                 linewidths=1.5)

    plt.title(f'Оптимальные научные кластеры за {year} год\n(показаны топ-{num_clusters} кластеров)',
              fontsize=16,
              pad=20)
    plt.legend(fontsize=10,
              loc='center left',
              bbox_to_anchor=(1, 0.5),
              ncol=1)
    plt.axis('off')
    plt.margins(0.2)

    return pos

# Использование:
years_to_visualize = [2005]
pos = None
for year in years_to_visualize:
    G = temporal_graphs[year]
    clusters = temporal_clusters[year]
    pos = visualize_optimal_clusters(G, clusters, year, pos, num_clusters=10)  # Показываем 10 кластеров
    plt.savefig(f'clusters_{year}.png',
                dpi=300,
                bbox_inches='tight',
                pad_inches=0.5)
    plt.close()



