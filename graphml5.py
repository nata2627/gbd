# -*- coding: utf-8 -*-
"""GraphML5_ГордееваНГ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vrlMI36GNT4hhQ-keBXABHL4XzhJOcpo

**Графовые сверточные сети**
"""

# Commented out IPython magic to ensure Python compatibility.
# data and utils
!gdown 1EJuySt23fkPp8FNoW5xT59y__fm6J2l5 --folder
!gdown 1hMaTPtN-CmSoHDh4G4PRvwbeNncV11UD --folder
!pip install dgl==1.0.0 # Install DGL
# Libraries
import torch
from torch.autograd import Variable
import torch.nn as nn
import collections
import time
import numpy as np
import sys
sys.path.insert(0, 'lib/')
# %load_ext autoreload
# %autoreload 2
import pickle
import dgl
from dgl.data import MiniGCDataset
import matplotlib.pyplot as plt
import networkx as nx
from torch.utils.data import DataLoader
from lib.utils import Dictionary, MoleculeDataset, MoleculeDGL, Molecule

"""# ChebNets

## MNIST
"""

# Load small MNIST
[train_data, train_label, test_data, test_label] = torch.load('datasets/MNIST_1k.pt')
print('train_data',train_data.size())
print('train_label',train_label.size())
print('test_data',test_data.size())
print('test_label',test_label.size())

"""## Compute coarsened graphs

"""

from lib.grid_graph import grid_graph
from lib.coarsening import coarsen
from lib.coarsening import lmax_L
from lib.coarsening import perm_data
from lib.coarsening import rescale_L

# Construct grid graph
t_start = time.time()
grid_side = 28  # Each image is 28 * 28
number_edges = 8  # Each pixel has eight neighbors
A = grid_graph(grid_side, number_edges, 'euclidean') # create graph of Euclidean grid

# Compute coarsened graphs
num_coarsening_levels = 4
L, perm = coarsen(A, num_coarsening_levels)

# Compute largest eigenvalue of graph Laplacians
lmax = []
for i in range(num_coarsening_levels):
    lmax.append(lmax_L(L[i]))
print('lmax: ' + str([lmax[i] for i in range(num_coarsening_levels)]))

# Reindex nodes to satisfy a binary tree structure
train_data = perm_data(train_data, perm)
test_data = perm_data(test_data, perm)
train_data = torch.tensor(train_data).float()
test_data = torch.tensor(test_data).float()
print(train_data.size())
print(test_data.size())

print('Execution time: {:.2f}s'.format(time.time() - t_start))
del perm

"""### Задание 1: Реализуйте сеть ChebNet на основе архитектуры CNN LeNet-5

- Первый слой: CL с 32 признаками
- Второй слой: MaxPooling для уменьшения размера графа в 4 раза
- Третий слой: CL с 64 признаками
- Четвертый слой: MaxPooling для уменьшения размера графа в 4 раза
- Пятый слой : Полностью связный (или линейный) слой с 512 признаками
- Последний слой : Полностью подключенный (или линейный) слой с 10 выходными значениями для 10 классов.

Инструкции:

- Шаг 1: Определите архитектуру модели с помощью конструктора `def __init__()`.
  - Используйте [torch.nn.Linear()](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) для линейного преобразования размерности признаков.
  - Преобразуйте лапласиан SciPy разреженного графа и его огрубленные версии в разреженные матрицы PyTorch с помощью [torch.sparse.FloatTensor(indices, data, shape)](https://pytorch.org/docs/stable/sparse.html).
  - Для матрицы [scipy.sparse.coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) L, `L.row`, `L.col` и `L.data` - индексы строк, индексы столбцов и веса для каждого ребра, соответственно.

- Шаг 2: Определите слои MaxPooling в `def graph_max_pool()`, используя [torch.nn.MaxPool1d()](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d).
  - Обратите внимание, что объединение происходит по размерности узла (аналогично одномерной последовательности), а не по размерности признака.
  - Используйте [torch.permute()](https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute), чтобы изменить порядок размерностей тензора для применения `torch.nn.MaxPool1d()`, и [.contiguous()](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous), чтобы обеспечить смежную компоновку памяти для целевого тензора.

- Шаг 3: Реализуем свертку ChebNet в `def graph_conv_cheby()`.
  - Вычислите явно первые два члена Чебышева, а затем закодируйте рекурсивную формулу.
  - Используйте [torch.sparse.mm()](https://pytorch.org/docs/stable/generated/torch.sparse.mm.html#torch.sparse.mm) для выполнения матричного умножения между двумя разреженными матрицами, что занимает меньше памяти, чем [torch.mm()](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm).
  - Признаки узлов интегрированы в первые два терма, поэтому последующие термы автоматически содержат признаки узлов через рекурсивные итерации.
  - Объедините результаты всех промежуточных порядков Чебышева, чтобы получить выход, линейно преобразовав их для соответствия размерности входа следующего слоя.

- Шаг 4: Реализуйте forward pass в `def forward()`.
  - Для CL1 и CL2, `graph_conv_cheby -> torch.relu (нелинейная активация) -> graph_max_pool`.
  - Для FC1 и FC2, `fc1 -> torch.relu (нелинейная активация) -> dropout -> fc2`.
"""

# class definition
class ChebNet_LeNet5(nn.Module):
    def __init__(self, net_parameters, Ls, lmax):
        super().__init__()
        # parameters
        # D: dimension of input features
        # CL1_F, CL2_F: dimensions of output from the First and Third layers
        # CL1_K, CL2_K: orders of Chebyshev terms used in CL1 and CL2
        # FC1_F, FC2_F: dimensions of output from the Fifth and Last layers
        # FC1Fin: dimensions of input for the Fifth layer
        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters
        FC1Fin = CL2_F*(D//16)

        # Step 1.1: Using the input and output dims at CL1, CL2, FC1, FC2 layers, define the corresponding feature transformation functions
        # graph CL1
        self.cl1 = nn.Linear(CL1_K, CL1_F)  # ВАШ КОД
        self.CL1_K = CL1_K # ВАШ КОД
        # graph CL2
        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F)  # ВАШ КОД
        self.CL2_K = CL2_K # ВАШ КОД
        # FC1
        self.fc1 = nn.Linear(FC1Fin, FC1_F) # ВАШ КОД
        self.FC1Fin = FC1Fin # ВАШ КОД
        # FC2
        self.fc2 = nn.Linear(FC1_F, FC2_F) # ВАШ КОД
        self.CL1_F = CL1_F
        self.CL2_F = CL2_F

        # Compute the pytorch Laplacian and its coarsened versions
        self.L = []
        for i in range(num_coarsening_levels+1):
            L = Ls[i]
            # rescale Laplacian: shift L into the definition domain of Chebyshev expansion λ ∈ [-1, 1]
            lmax = lmax_L(L)
            L = rescale_L(L, lmax)
            # convert scipy sparse matric L to pytorch
            L = L.tocoo()

            # Step 1.2: Convert each L into a pytorch sparse tensor
            indices = np.column_stack((L.row, L.col)).T # ВАШ КОД
            indices = indices.astype(np.int64) # ВАШ КОД
            indices = torch.from_numpy(indices) # ВАШ КОД
            indices = indices.type(torch.LongTensor) # ВАШ КОД
            L_data = L.data.astype(np.float32) # ВАШ КОД
            L_data = torch.from_numpy(L_data) # ВАШ КОД
            L_data = L_data.type(torch.FloatTensor) # ВАШ КОД
            L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))

            L = Variable( L , requires_grad=False)
            self.L.append(L)

    # Max pooling of size p (p must be a power of 2)
    def graph_max_pool(self, x, p):
        # B, V, F = x.shape
        # B = batch size
        # V = num vertices
        # F = num features
        if p > 1:

            # Step 2: Exchange the vertex dim and feature dim, do pooling along the last dim.
            x = x.permute(0,2,1).contiguous() # ВАШ КОД  # x = B x F x V
            x = nn.MaxPool1d(p)(x) # ВАШ КОД  # B x F x V/p
            x = x.permute(0,2,1).contiguous() # ВАШ КОД  # x = B x V/p x F

            return x
        else:
            return x

    # Graph convolution layer
    def graph_conv_cheby(self, x, cl, L, Fout, K):
        # parameters
        # B = batch size
        # V = num vertices
        # Fin = num input features
        # Fout = num output features
        # K = Chebyshev order and support size
        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin)

        # Transform to Chebyshev basis

        # Step 3.1: Compute the first two Chebyshev terms, integrate 'x', concatenate them
        # The first Chebyshev term: T_0(L)=I
        # The second Chebyshev term: T_1(L)=L
        x0 = x.permute(1,2,0).contiguous() # ВАШ КОД # V x Fin x B
        x0 = x0.view([V, Fin*B]) # ВАШ КОД # V x Fin*B

        # x: Concatenate the outputs from each Chebyshev term
        x = x0.unsqueeze(0)                 # 1 x V x Fin*B
        if K > 1:
            x1 = torch.sparse.mm(L,x0)             # V x Fin*B
            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B

        for k in range(2, K):
            # Step 3.2: Apply recursive formula, concatenate results, update x0, x1 as x1, x2
            # The following Chebyshev terms: T_2(L)=2*LT_1(L)-T_0(L)
            x2 = 2 * torch.sparse.mm(L, x1) - x0  # ВАШ КОД
            x = torch.cat((x, x2.unsqueeze(0)),0) # ВАШ КОД  # (k+1) x V x Fin*B
            x0, x1 = x1, x2
        x = x.view([K, V, Fin, B])           # K x V x Fin x B
        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K
        x = x.view([B*V, Fin*K])             # B*V x Fin*K

        # Step 3.3: Linear transform Fin features to obtain Fout features
        x = cl(x) # ВАШ КОД # B*V x Fout

        x = x.view([B, V, Fout])             # B x V x Fout
        return x

    def forward(self, x):
        # graph CL1
        x = x.unsqueeze(2) # B x V x Fin=1, images in MNIST only have one channel/feature
        x = self.graph_conv_cheby(x, self.cl1, self.L[0], self.CL1_F, self.CL1_K)
        x = torch.relu(x)
        x = self.graph_max_pool(x, 4)
        # graph CL2

        # Step 4.1: Implement CL2
        x = self.graph_conv_cheby(x, self.cl2, self.L[2], self.CL2_F, self.CL2_K) # ВАШ КОД)
        x = torch.relu(x)
        x = self.graph_max_pool(x, 4)

        # FC1
        x = x.view(-1, self.FC1Fin) # resize the tensor

        # Step 4.2: Implement FC1
        x = self.fc1(x) # ВАШ КОД
        x = torch.relu(x)

        # FC2
        x = self.fc2(x)
        return x

    def update_learning_rate(self, optimizer, lr): # Adjust the learning rate based on the number of epochs
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        return optimizer

"""### Задание 2: Выведите основную информацию ChebNet и протестируйте прямой и обратный проходы одним батчем.

Инструкции:

Шаг 1: Подсчитайте количество параметров модели.
  - Для pytorch-модели `net`, [net.parameters()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) возвращает итератор по параметрам модуля. [torch.numel()](https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel) возвращает общее количество элементов во входном тензоре.

Шаг 2: Реализуйте функцию потерь, включающую стандартную потерю классификации кросс-энтропии и потерю регуляризации L2 для обучаемых параметров сети.
  - Используйте [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) для вычисления кросс-энтропийных потерь между входными логитами и целевыми метками.
  - Используйте `for param in net.parameters():` для доступа к обучаемым параметрам для потери L2 регуляризации, определенной как L$(w_1,w_2)=\sum_k\|w_k\|^2$.  

"""

# network parameters
D = train_data.shape[1]
CL1_F = 32
CL1_K = 25
CL2_F = 64
CL2_K = 25
FC1_F = 512
FC2_F = 10
net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]

# instantiate ChebNet
net = ChebNet_LeNet5(net_parameters, L, lmax)
print(net)

def display_num_param(net):
    nb_param = 0

    # Step 1: Count the number of model parameters
    for param in net.parameters():
        nb_param += torch.numel(param)# ВАШ КОД

    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))
    return nb_param/1e6

display_num_param(net)

# extract one batch
batch_size = 10
indices = torch.randperm(train_data.shape[0])
batch_idx = indices[:batch_size]
print('batch_idx: ',batch_idx)
train_x, train_y = train_data[batch_idx,:], train_label[batch_idx]

# Forward
y = net(train_x)

# backward
def loss_reg(lossCE, net, y, y_target, l2_regularization):
    CE_loss = 0.0
    l2_loss = 0.0

    # Step 2: Compute CE_loss and l2_loss
    CE_loss = lossCE(y, y_target)
    for param in net.parameters():
        data = param* param # ВАШ КОД
        l2_loss += data.sum() # ВАШ КОД

    loss = 0.5* l2_regularization* l2_loss + CE_loss
    return loss

lossCE = nn.CrossEntropyLoss()
l2_regularization = 1e-3
loss = loss_reg(lossCE, net, y, train_y, l2_regularization)
loss.backward()

# Update
learning_rate = 0.05
optimizer = torch.optim.SGD( net.parameters(), lr=learning_rate, momentum=0.9 )
optimizer.zero_grad()
optimizer.step()

"""### Задание 3: Обучение ChebNet

Инструкции:

Шаг 1: Инициализируйте модель, оптимизатор и функцию потерь.

Шаг 2: Повторите цикл обучения `num_epochs` раз.
- В начале новой эпохи перемешайте выборки, обнулите значение потерь и точность и переведите `сеть` в режим обучения: [net.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train).
- В течение каждой эпохи используется последовательность инструкций `пакетные данные -> модель -> выходные логиты -> потери -> обратное распространение -> обновление параметров -> оценка`.
- Градиент потерь по параметрам сети вычисляется автоматически с помощью [loss.backward()](https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward).
- Один шаг обновления значений параметров выполняется с помощью [optimizer.step()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step). Не забывайте обнулять градиент в каждом мини-партии с помощью [optimizer.zeroo_grad()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad).
- После каждой эпохи обновляйте скорость обучения и оценивайте точность тестового набора с помощью [torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad), чтобы отключить вычисление градиента, и [net.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module), чтобы перевести `сеть` в режим оценки.
"""

# network parameters
D = train_data.shape[1]
CL1_F = 32
CL1_K = 25
CL2_F = 64
CL2_K = 25
FC1_F = 512
FC2_F = 10
net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]

# instantiate ChebNet
net = ChebNet_LeNet5(net_parameters, L, lmax)
print(net)
display_num_param(net)

# optimization parameters
lr = 0.05 # learning_rate
init_lr = lr
l2_regularization = 1e-3
batch_size = 100
num_epochs = 20
num_train_data = train_data.shape[0]
nb_iter = int(num_epochs * num_train_data) // batch_size
print('num_epochs=',num_epochs,', num_train_data=',num_train_data,', nb_iter=',nb_iter)

# Optimizer
# Step 1: Initialize loss, optimizer and evaluation
lossCE =  nn.CrossEntropyLoss() # ВАШ КОД
optimizer = torch.optim.Adam(net.parameters(), lr=lr) # ВАШ КОД

def evaluation(y_predicted, y_label):
    _, class_predicted = torch.max(y_predicted, 1)
    return 100.0* (class_predicted == y_label).sum()/ y_predicted.size(0)

# loop over epochs
num_data = 0
for epoch in range(num_epochs):

    # reshuffle
    indices = torch.randperm(num_train_data)

    # reset time
    t_start = time.time()

    # extract batches
    running_loss = 0.0
    running_accuray = 0
    running_total = 0
    net.train()
    for idx in range(0,num_train_data,batch_size):

        # extract batches
        train_x, train_y = train_data[idx:idx+batch_size,:], train_label[idx:idx+batch_size]

        # Forward
        y = net(train_x)

        # backward
        loss = loss_reg(lossCE, net, y, train_y, l2_regularization)

        # Step 2: Apply backward propagation and update net parameters
        loss.backward() # ВАШ КОД

        # Accuracy
        acc_train = evaluation(y.detach(), train_y)

        optimizer.step()
        optimizer.zero_grad()

        # loss, accuracy
        num_data += batch_size
        running_loss += loss.detach()
        running_accuray += acc_train
        running_total += 1

    # print
    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' %
          (epoch+1, running_loss/running_total, running_accuray/running_total, time.time()-t_start, lr))

    # update learning rate
    lr = init_lr * pow( 0.95 , float(num_data// num_train_data) )
    optimizer = net.update_learning_rate(optimizer, lr)

    # Test set
    with torch.no_grad():
        net.eval()
        running_accuray_test = 0
        running_total_test = 0
        num_test_data = test_data.size(0)
        indices_test = torch.arange(num_test_data)
        t_start_test = time.time()
        for idx in range(0,num_test_data,batch_size):
            test_x, test_y = test_data[idx:idx+batch_size,:], test_label[idx:idx+batch_size]
            y = net(test_x)
            acc_test = evaluation(y.detach(), test_y)
            running_accuray_test += acc_test
            running_total_test += 1
        t_stop_test = time.time() - t_start_test
        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))

"""# GCNs

## Визуализация графа
"""

dataset = MiniGCDataset(8, 10, 20) # DGL artificial dataset

# visualise the 8 classes of graphs
for c in range(8):
    graph, label = dataset[c]
    #fig, ax = plt.subplots()
    fig, ax = plt.subplots(figsize=(4,4))
    nx.draw(graph.to_networkx(), ax=ax)
    ax.set_title('Class: {:d}'.format(label))
    plt.show()

"""## Generate train, val and test datasets

### Вопрос 4: Добавьте признак узла с узлом внутренней степени

Можете использовать методы `.in_degrees()`, `.view()`, `.float()`.
"""

# Add node features to graphs
def add_node_features(dataset):
    for (graph,_) in dataset:

        # node feature = node in-degree
        # graph.ndata['feat'].size()=(num_nodes,1) and graph.ndata['feat'].type()=torch.FloatTensor
        graph.ndata['feat'] = graph.in_degrees().view(-1, 1).float()
    return dataset

# Generate train, val and test graph datasets
trainset = MiniGCDataset(350, 10, 20)
testset = MiniGCDataset(100, 10, 20)
valset = MiniGCDataset(100, 10, 20)
trainset = add_node_features(trainset)
testset = add_node_features(testset)
valset = add_node_features(valset)
print(trainset[0])

"""### Задание 6: Определите функцию collate для создания батча графов DGL и проверьте ее

Создайте батча коллекции графов DGL в один граф для более эффективного вычисления.
Можете использовать модуль DGL [dgl.batch()](https://docs.dgl.ai/generated/dgl.batch.html#dgl.batch).

"""

# collate function prepares a batch of graphs and labels
def collate(samples):
    # Input sample is a list of pairs (graph, label)
    graphs, labels = map(list, zip(*samples)) # "graphs" is a list of "batch_size" DGL graphs
                                              # "labels" is a list of "batch_size" class labels

    # Create the DGL batch of graphs, which is equivalent to build a block diagonal matrix with all graphs in the batch
    batch_graphs = dgl.batch(graphs)

    batch_labels = torch.tensor(labels) # batch of labels
    return batch_graphs, batch_labels

# Generate a batch of graphs
batch_size = 10
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)
batch_graphs, batch_labels = list(train_loader)[0]
print(batch_graphs)
print(batch_labels)
batch_x = batch_graphs.ndata['feat']
print('batch_x:',batch_x.size())

"""### Задание 7: Спроектировать класс сетей GCN с помощью DGL

Уравнение обновления узлов:  
\begin{eqnarray}
h_i^{\ell+1} &=& h_i^{\ell} + \text{ReLU} \left( \frac{1}{\sqrt{d_i}} \sum_{j\sim i} \frac{1}{\sqrt{d_j}} W^\ell h_j^{\ell} \right)
\end{eqnarray}

Инструкции:

Шаг 1: Передаем признаки узла и ребра по ребрам (src/j => dst/i) в `message_func()`.
  - `edges.src['xx']` должен выбрать признак 'xx' из исходных узлов ребер.
    
Шаг 2: Функция `reduce_func()` собирает все сообщения={Whj, dj} в узел dst/i с шага 1.
  - `nodes.mailbox['yy']` собирает признак 'yy' из соседних узлов.
"""

# MLP layer for classification
class MLP_layer(nn.Module):

    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers
        super(MLP_layer, self).__init__()
        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]
        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))
        self.FC_layers = nn.ModuleList(list_FC_layers)
        self.L = L

    def forward(self, x):
        y = x
        for l in range(self.L):
            y = self.FC_layers[l](y)
            y = torch.relu(y)
        y = self.FC_layers[self.L](y)
        return y


# class of GatedGCN layer
class GCN_layer(nn.Module):

    def __init__(self, input_dim, output_dim):
        super(GCN_layer, self).__init__()
        self.W = nn.Linear(input_dim, output_dim, bias=True)

    # Step 1 of message-passing with DGL:
    #   Node feature and edge features are passed along edges (src/j => dst/i)
    def message_func(self, edges):

        # Step 1: Pass the messages Wh_j and d_j (in-degree_j) to the node i
        Whj = edges.src['Wh'] # ВАШ КОД
        dj = edges.src['d'] # ВАШ КОД

        return {'Whj' : Whj, 'dj' : dj}

    # Step 2 of message-passing with DGL:
    #   Reduce function collects all messages={Whj, dj} sent to node dst/i with Step 1
    def reduce_func(self, nodes):

        # Step 2: Node i receives messages Wh_j and d_j (in-degree_j)
        Whj = nodes.mailbox['Whj'] # ВАШ КОД
        dj = nodes.mailbox['dj'] # ВАШ КОД

        inv_sqrt_dj = torch.pow(dj, -0.5) # 1/sqrt(d_j)
        h = torch.sum( inv_sqrt_dj * Whj, dim=1 ) # hi = sum_j 1/sqrt(d_j) * Bhj
        return {'h' : h} # return update node feature hi

    def forward(self, g, h):
        h_in = h # residual connection
        g.ndata['Wh'] = self.W(h) # linear transformation
        d = g.in_degrees().view(-1, 1).float() # in-degree d_i
        g.ndata['d'] = d
        g.update_all(self.message_func,self.reduce_func) # update the node feature with DGL
        h = g.ndata['h'] # collect the node output of graph convolution
        inv_sqrt_d = torch.pow(d, -0.5) # 1/ sqrt(d_i)
        h = inv_sqrt_d * h # h_i / sqrt(d_i)
        h = torch.relu(h) # non-linear activation
        h = h_in + h # residual connection
        return h


class GCN_net(nn.Module):

    def __init__(self, net_parameters):
        super(GCN_net, self).__init__()
        input_dim = net_parameters['input_dim']
        hidden_dim = net_parameters['hidden_dim']
        output_dim = net_parameters['output_dim']
        L = net_parameters['L']
        self.embedding_h = nn.Linear(input_dim, hidden_dim)
        self.GCN_layers = nn.ModuleList([ GCN_layer(hidden_dim, hidden_dim) for _ in range(L) ])
        self.MLP_layer = MLP_layer(hidden_dim, output_dim)

    def forward(self, g, h):

        # input embedding
        h = self.embedding_h(h)

        # graph convnet layers
        for GCNlayer in self.GCN_layers:
            h = GCNlayer(g,h)

        # MLP classifier
        g.ndata['h'] = h
        y = dgl.mean_nodes(g,'h')
        y = self.MLP_layer(y)

        return y

    def loss(self, y_scores, y_labels):
        loss = nn.CrossEntropyLoss()(y_scores, y_labels)
        return loss

    def accuracy(self, scores, targets):
        scores = scores.detach().argmax(dim=1)
        acc = (scores==targets).float().sum().item()
        return acc

    def update(self, lr):
        update = torch.optim.Adam( self.parameters(), lr=lr )
        return update


# Instantiate one network (testing)
net_parameters = {}
net_parameters['input_dim'] = 1
net_parameters['hidden_dim'] = 128
net_parameters['output_dim'] = 8 # nb of classes
net_parameters['L'] = 4
net = GCN_net(net_parameters)
print(net)

def display_num_param(net):
    nb_param = 0
    for param in net.parameters():
        nb_param += param.numel()
    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))
    return nb_param/1e6
_ = display_num_param(net)

"""### Задание 8: Запустите обучение"""

def run_one_epoch(net, data_loader, train=True):
    if train:
        net.train()
    else:
        net.eval()
    epoch_loss = 0
    epoch_acc = 0
    nb_data = 0
    gpu_mem = 0
    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):
        batch_x = batch_graphs.ndata['feat']
        batch_labels = batch_labels
        batch_scores = net.forward(batch_graphs, batch_x)
        loss = net.loss(batch_scores, batch_labels)
        if train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        epoch_loss += loss.detach().item()
        epoch_acc += net.accuracy(batch_scores,batch_labels)
        nb_data += batch_labels.size(0)
    epoch_loss /= (iter + 1)
    epoch_acc /= nb_data
    return epoch_loss, epoch_acc


# dataset loaders
train_loader = DataLoader(trainset, batch_size=50, shuffle=True, collate_fn=collate)
test_loader = DataLoader(testset, batch_size=50, shuffle=False, collate_fn=collate)
val_loader = DataLoader(valset, batch_size=50, shuffle=False, drop_last=False, collate_fn=collate)

# Instantiate one network
net_parameters = {}
net_parameters['input_dim'] = 1
net_parameters['hidden_dim'] = 128
net_parameters['output_dim'] = 8 # nb of classes
net_parameters['L'] = 4
net = GCN_net(net_parameters)
display_num_param(net)

# optimizer
optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)

# training loop
for epoch in range(50):
    start = time.time()
    epoch_train_loss, epoch_train_acc = run_one_epoch(net, train_loader, True)
    with torch.no_grad():
        epoch_test_loss, epoch_test_acc = run_one_epoch(net, test_loader, False)
        epoch_val_loss, epoch_val_acc = run_one_epoch(net, val_loader, False)
    if not epoch%2:
        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}, val_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss, epoch_val_loss))
        print('                      train_acc: {:.4f}, test_acc: {:.4f}, val_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc, epoch_val_acc))