# -*- coding: utf-8 -*-
"""GraphML2_ГордееваНГ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JpherzngELzb_7goUBlkuo39cSi5Izro

# Init
"""

# load data and utils
!gdown 1ahgakQeKKLHXV3rIfXGeN53sm8F6A9E- --folder
!gdown 1SLTpRHjJtgNYZukwXUAxBfH0ee4upv7i --folder

# Commented out IPython magic to ensure Python compatibility.
# Load libraries
!pip install -qU PyMetis
!pip install -qU python-louvain
import community.community_louvain as community_louvain
import networkx as nx
import pymetis
from matplotlib import pyplot
import numpy as np
import scipy.io
#%matplotlib notebook
# %matplotlib inline
import matplotlib.pyplot as plt
from IPython.display import display, clear_output
import time
import sys; sys.path.insert(0, 'lib/')
# %load_ext autoreload
# %autoreload 2
from lib.utils import construct_kernel
from lib.utils import compute_kernel_kmeans_EM
from lib.utils import compute_kernel_kmeans_spectral
from lib.utils import compute_purity
from lib.utils import construct_knn_graph
from lib.utils import compute_ncut
from lib.utils import compute_pcut
from lib.utils import graph_laplacian
from lib.utils import nldr_visualization
import warnings; warnings.filterwarnings("ignore")

"""# Standard k-means

## Gaussian Mixture Model (GMM)
"""

# Load raw data images
mat = scipy.io.loadmat('datasets/GMM.mat')
X = mat['X']
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt'] - 1; Cgt = Cgt.squeeze()
nc = len(np.unique(Cgt))
print(n,d,nc)

plt.figure(1)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, cmap='jet')
plt.title('Gaussian Mixture Model (GMM) -- Linearly separable data points')
plt.show()

"""Оцените влияния различных инициализаций на производительность k-Means

**Методы инициализации:**
* **Константная функция:** Вы можете использовать `numpy.ones()` для этой инициализации.
* **Случайная функция:** Рассмотрите `numpy.random.randint()` для случайной инициализации.
Рассмотрите, как эти методы инициализации влияют на результаты кластеризации.


"""

# Константная инициализация
n = X.shape[0]

# Функция для инициализации центроидов
def initialize_centroids_constant(nc, n):
    return np.ones(n)  # Константная инициализация

def initialize_centroids_random(nc, n):
    return np.random.randint(0, nc, size=n)  # Случайная инициализация

# Выбор метода инициализации
initialization_method = 'constant'  # или 'random'

if initialization_method == 'constant':
    C_kmeans = initialize_centroids_constant(nc, n)
elif initialization_method == 'random':
    C_kmeans = initialize_centroids_random(nc, n)

# Linear Kernel for standard K-Means
Ker = X.dot(X.T)
print(Ker.shape)

# Loop
Cold = np.ones([n])
diffC = 1e10
Theta = np.ones(n) # Same weight for each data
Theta = np.diag(Theta)
Ones = np.ones((1,n))
En_iters = []
Clusters_iters = []; Clusters_iters.append(C_kmeans)
k = 0
while (k<50) & (diffC>1e-2):

    # Update iteration
    k += 1

    # Distance Matrix D
    row = np.array(range(n))
    col = C_kmeans
    data = np.ones(n)
    F = scipy.sparse.csr_matrix((data, (row, col)), shape=(n, nc)).todense()
    O = np.diag( np.array( 1./ (Ones.dot(Theta).dot(F) + 1e-6) ).squeeze() )
    T = Ker.dot(Theta.dot(F.dot(O)))
    D = - 2* T + np.repeat( np.diag(O.dot((F.T).dot(Theta.dot(T))))[None,:] ,n,axis=0)

    # Extract clusters
    C_kmeans = np.array(np.argmin(D,1)).squeeze()
    Clusters_iters.append(C_kmeans)

    # L2 difference between two successive cluster configurations
    diffC = np.linalg.norm(C_kmeans-Cold)/np.linalg.norm(Cold)
    Cold = C_kmeans

    # K-Means energy
    En = np.multiply( (np.repeat(np.diag(Ker)[:,None],nc,axis=1) + D) , F)
    En_kmeans = np.sum(En)/n
    En_iters.append(En_kmeans)

print(k)

# Visualize k-means iterations
fig, ax = plt.subplots()
for k,C in enumerate(Clusters_iters):
    plt.scatter(X[:,0], X[:,1], s=10*np.ones(n), c=C, cmap='jet')
    plt.title('k-means clusters at iteration = ' + str(k+1) )
    display(fig)
    clear_output(wait=True)

# Visualize loss vs iteration
plt.figure(3)
plt.plot(En_iters)
plt.title('loss vs iteration')
plt.show()

# Случайная инициализация
n = X.shape[0]

# Функция для инициализации центроидов
def initialize_centroids_constant(nc, n):
    return np.zeros(n)  # Константная инициализация

def initialize_centroids_random(nc, n):
    return np.random.randint(0, nc, size=n)  # Случайная инициализация

# Выбор метода инициализации
initialization_method = 'random'

if initialization_method == 'constant':
    C_kmeans = initialize_centroids_constant(nc, n)
elif initialization_method == 'random':
    C_kmeans = initialize_centroids_random(nc, n)

# Linear Kernel for standard K-Means
Ker = X.dot(X.T)
print(Ker.shape)

# Loop
Cold = np.ones([n])
diffC = 1e10
Theta = np.ones(n) # Same weight for each data
Theta = np.diag(Theta)
Ones = np.ones((1,n))
En_iters = []
Clusters_iters = []; Clusters_iters.append(C_kmeans)
k = 0
while (k<50) & (diffC>1e-2):

    # Update iteration
    k += 1

    # Distance Matrix D
    row = np.array(range(n))
    col = C_kmeans
    data = np.ones(n)
    F = scipy.sparse.csr_matrix((data, (row, col)), shape=(n, nc)).todense()
    O = np.diag( np.array( 1./ (Ones.dot(Theta).dot(F) + 1e-6) ).squeeze() )
    T = Ker.dot(Theta.dot(F.dot(O)))
    D = - 2* T + np.repeat( np.diag(O.dot((F.T).dot(Theta.dot(T))))[None,:] ,n,axis=0)

    # Extract clusters
    C_kmeans = np.array(np.argmin(D,1)).squeeze()
    Clusters_iters.append(C_kmeans)

    # L2 difference between two successive cluster configurations
    diffC = np.linalg.norm(C_kmeans-Cold)/np.linalg.norm(Cold)
    Cold = C_kmeans

    # K-Means energy
    En = np.multiply( (np.repeat(np.diag(Ker)[:,None],nc,axis=1) + D) , F)
    En_kmeans = np.sum(En)/n
    En_iters.append(En_kmeans)

print(k)

# Visualize k-means iterations
fig, ax = plt.subplots()
for k,C in enumerate(Clusters_iters):
    plt.scatter(X[:,0], X[:,1], s=10*np.ones(n), c=C, cmap='jet')
    plt.title('k-means clusters at iteration = ' + str(k+1) )
    display(fig)
    clear_output(wait=True)

# Visualize loss vs iteration
plt.figure(3)
plt.plot(En_iters)
plt.title('loss vs iteration')
plt.show()

"""Таким образом, рандомная инициализация работает лучше: ее производительность выше, а также выше точность по сравнению с константной (верно поделил на кластеры по сравнению с константной)

## Two concentric circles
"""

# Load raw data images
mat = scipy.io.loadmat('datasets/two_circles.mat')
X = mat['X']
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt'] - 1; Cgt = Cgt.squeeze()
nc = len(np.unique(Cgt))
print(n,d,nc)

plt.figure(10)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, cmap='jet')
plt.title('Distribution of two circle distributions -- Non-linear data points')
plt.show()

"""Оцените эффективность k-Means при различных инициализациях

Возможно ли определить функцию инициализации, которая успешно разделяет два класса в этом наборе данных?

Оцените эффективность k-средних на этом наборе данных.

"""

# Случайная инициализация
n = X.shape[0]

def initialize_centroids_constant(nc):
    return np.ones(n)  # Константная инициализация

def initialize_centroids_random(nc):
    return np.random.randint(0, nc, size=n)  # Случайная инициализация

# Выбор метода инициализации
initialization_method = 'random'  # 'constant' или 'random'

if initialization_method == 'constant':
    C_kmeans = initialize_centroids_constant(nc)
elif initialization_method == 'random':
    C_kmeans = initialize_centroids_random(nc)

# Linear Kernel for standard K-Means
Ker = X.dot(X.T)
print(Ker.shape)

# Loop
Cold = np.ones([n])
diffC = 1e10
Theta = np.ones(n) # Equal weight for each data
Theta = np.diag(Theta)
Ones = np.ones((1,n))
En_iters = []
Clusters_iters = []; Clusters_iters.append(C_kmeans)
k = 0
while (k<10) & (diffC>1e-2):

    # Update iteration
    k += 1
    #print(k)

    # Distance Matrix D
    row = np.array(range(n))
    col = C_kmeans
    data = np.ones(n)
    F = scipy.sparse.csr_matrix((data, (row, col)), shape=(n, nc)).todense()
    O = np.diag( np.array( 1./ (Ones.dot(Theta).dot(F) + 1e-6) ).squeeze() )
    T = Ker.dot(Theta.dot(F.dot(O)))
    D = - 2* T + np.repeat( np.diag(O.dot((F.T).dot(Theta.dot(T))))[None,:] ,n,axis=0)
    #print(D.shape)

    # Extract clusters
    C_kmeans = np.array(np.argmin(D,1)).squeeze()
    Clusters_iters.append(C_kmeans)

    # L2 difference between two successive cluster configurations
    diffC = np.linalg.norm(C_kmeans-Cold)/np.linalg.norm(Cold)
    Cold = C_kmeans

    # K-Means energy
    En = np.multiply( (np.repeat(np.diag(Ker)[:,None],nc,axis=1) + D) , F)
    En_kmeans = np.sum(En)/n
    En_iters.append(En_kmeans)

print(k)

# Visualize k-means iterations
fig, ax = plt.subplots()
for k,C in enumerate(Clusters_iters):
    plt.scatter(X[:,0], X[:,1], s=10*np.ones(n), c=C, cmap='jet')
    plt.title('k-means clusters at iteration = ' + str(k+1) )
    display(fig)
    clear_output(wait=True)

# Visualize loss vs iteration
plt.figure(12)
plt.plot(En_iters)
plt.title('loss vs iteration')
plt.show()

# Константная инициализация
n = X.shape[0]

def initialize_centroids_constant(nc):
    return np.ones(n)  # Константная инициализация

def initialize_centroids_random(nc):
    return np.random.randint(0, nc, size=n)  # Случайная инициализация

# Выбор метода инициализации
initialization_method = 'constant'  # 'constant' или 'random'

if initialization_method == 'constant':
    C_kmeans = initialize_centroids_constant(nc)
elif initialization_method == 'random':
    C_kmeans = initialize_centroids_random(nc)

# Linear Kernel for standard K-Means
Ker = X.dot(X.T)
print(Ker.shape)

# Loop
Cold = np.ones([n])
diffC = 1e10
Theta = np.ones(n) # Equal weight for each data
Theta = np.diag(Theta)
Ones = np.ones((1,n))
En_iters = []
Clusters_iters = []; Clusters_iters.append(C_kmeans)
k = 0
while (k<10) & (diffC>1e-2):

    # Update iteration
    k += 1
    #print(k)

    # Distance Matrix D
    row = np.array(range(n))
    col = C_kmeans
    data = np.ones(n)
    F = scipy.sparse.csr_matrix((data, (row, col)), shape=(n, nc)).todense()
    O = np.diag( np.array( 1./ (Ones.dot(Theta).dot(F) + 1e-6) ).squeeze() )
    T = Ker.dot(Theta.dot(F.dot(O)))
    D = - 2* T + np.repeat( np.diag(O.dot((F.T).dot(Theta.dot(T))))[None,:] ,n,axis=0)
    #print(D.shape)

    # Extract clusters
    C_kmeans = np.array(np.argmin(D,1)).squeeze()
    Clusters_iters.append(C_kmeans)

    # L2 difference between two successive cluster configurations
    diffC = np.linalg.norm(C_kmeans-Cold)/np.linalg.norm(Cold)
    Cold = C_kmeans

    # K-Means energy
    En = np.multiply( (np.repeat(np.diag(Ker)[:,None],nc,axis=1) + D) , F)
    En_kmeans = np.sum(En)/n
    En_iters.append(En_kmeans)

print(k)

# Visualize k-means iterations
fig, ax = plt.subplots()
for k,C in enumerate(Clusters_iters):
    plt.scatter(X[:,0], X[:,1], s=10*np.ones(n), c=C, cmap='jet')
    plt.title('k-means clusters at iteration = ' + str(k+1) )
    display(fig)
    clear_output(wait=True)

# Visualize loss vs iteration
plt.figure(12)
plt.plot(En_iters)
plt.title('loss vs iteration')
plt.show()

"""Нельзя определить точно, так как при различных инициализациях значение ошибки достигает своего минимального значения уже на первой итерации, а с ростом итераций ошибка больше не падает. Следовательно, правильно кластеризовать не имеется возможным.

# Kernel k-means
"""

# Load two-circle dataset
mat = scipy.io.loadmat('datasets/two_circles.mat')
X = mat['X'] # (2000, 2), numpy.ndarray
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt']-1; Cgt = Cgt.squeeze() # (2000,)
nc = len(np.unique(Cgt)) # 2
print('n,d,nc:',n,d,nc)

plt.figure(1)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, cmap='jet')
plt.title('Distribution of two circle distributions -- Non-linear data points')
plt.show()

"""## Linear k-Means using Expectation-Maximization

Оцените производительность *Linear* k-Means с помощью Expectation-Maximization (EM) с несколькими случайными инициализациями.
Используйте следующий код: `compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)` с входными аргументами:
* nc : Количество кластеров.
* Ker : матрица ядра размером n x n, где n - количество точек данных.
* Theta : Весовая матрица размера n x n, обычно диагональная матрица с весами каждой точки данных.
* n_trials : Количество запусков для kernel k-means. Функция возвращает решение с минимальной конечной энергией.

Сколько прогонов необходимо для получения корректного решения?
"""

# Run standard/linear k-means
Theta = np.ones(n) # Same weight for all data

# Compute linear kernel for standard k-means
Ker = construct_kernel(X, 'linear') # (2000, 2000)
print(Ker.shape)

# standard k-means
n_trials = 20
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(2)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Standard k-means solution. Accuracy: ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy: ' + str(En_kmeans)[:5])
plt.show()

# Run standard/linear k-means
Theta = np.ones(n) # Same weight for all data

# Compute linear kernel for standard k-means
Ker = construct_kernel(X, 'linear') # (2000, 2000)
print(Ker.shape)

# standard k-means
n_trials = 10
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(2)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Standard k-means solution. Accuracy: ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy: ' + str(En_kmeans)[:5])
plt.show()

"""код выполняет линейный k-Means с использованием EM алгоритма. Мы изменили количество запусков (n_trials), чтобы оценить стабильность решения.
 от 10 до 20 запусков достаточно для получения корректного решения.

## Non-Linear k-Means using Expectation-Maximization

Оцените производительность *нелинейного* k-Means с помощью EM с несколькими случайными инициализациями

Сколько прогонов необходимо для получения корректного решения?
"""

# Run kernel/non-linear k-means with EM approach

# Compute linear Kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with EM approach
n_trials = 10
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with EM approach. Accuracy= ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy= ' + str(En_kmeans)[:5])
plt.show()

# Run kernel/non-linear k-means with EM approach

# Compute linear Kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with EM approach
n_trials = 20
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with EM approach. Accuracy= ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy= ' + str(En_kmeans)[:5])
plt.show()

# Run kernel/non-linear k-means with EM approach

# Compute linear Kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with EM approach
n_trials = 25
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with EM approach. Accuracy= ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy= ' + str(En_kmeans)[:5])
plt.show()

# Run kernel/non-linear k-means with EM approach

# Compute linear Kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with EM approach
n_trials = 30
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with EM approach. Accuracy= ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy= ' + str(En_kmeans)[:5])
plt.show()

# Run kernel/non-linear k-means with EM approach

# Compute linear Kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with EM approach
n_trials = 40
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, n_trials)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with EM approach. Accuracy= ' + str(compute_purity(C_kmeans,Cgt,nc))[:5] +
         ', Energy= ' + str(En_kmeans)[:5])
plt.show()

"""Для корректного решения достаточно 25 прогонов, на 20 прогоне при стабильном energy точность ниже, на 40 прогоне модель переобучается, ее качество начинает падать.

## Non-Linear k-Means using the Spectral technique

Оцените производительности нелинейных k-Means с помощью спектрального метода

Используйте функцию `compute_kernel_kmeans_spectral(nc, Ker, Theta)` со следующими входными аргументами:
* nc : Количество кластеров.
* Ker : матрица ядра размером n x n, где n - количество точек данных.
* Theta : Весовая матрица размера n x n, диагональная матрица, содержащая веса каждой точки данных.

Обратите внимание, что у этой функции нет входного аргумента `n_trials`. Почему так?
"""

# Run kernel/non-linear k-means with spectral approach

# Compute linear kernel for standard k-means
Ker = construct_kernel(X, 'kNN_gaussian', 100)
print(Ker.shape)

# Kernel k-means with spectral approac

C_kmeans, En_kmeans = compute_kernel_kmeans_spectral(nc, Ker, Theta)

# Plot
plt.figure(4)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, cmap='jet')
plt.title('Kernel k-means solution with spectral approach. Accuracy= ' +
          str(compute_purity(C_kmeans,Cgt,nc))[:5] + ' Energy= ' + str(En_kmeans)[:5])
plt.show()

"""Точность наиболее высокая. Спектральный метод предлагает мощный инструмент для кластеризации нелинейно разделимых данных и не требует многократных запусков для достижения стабильного результата

# Metis
"""

# Load USPS dataset
mat = scipy.io.loadmat('datasets/USPS.mat')
W = mat['W'] # scipy.sparse._csc.csc_matrix
n = W.shape[0]
Cgt = mat['Cgt']-1; Cgt = Cgt.squeeze()
nc = len(np.unique(Cgt))
print('n,nc:',n,nc)

"""Разбиение реального графа изображений USPS с помощью Metis

После разбиения графа с помощью Metis визуализируйте его с помощью кластеров, представленных разными цветами.

Выявляют ли полученные кластеры какие-либо заметные закономерности?

"""

# Run Metis with PyMetis
num_parts = nc
G_nx = nx.from_scipy_sparse_array(W)
start = time.time()
_, part_vert = pymetis.part_graph(num_parts, G_nx)
print('Time(sec) : %.3f' % (time.time()-start) )
C_metis_pyg = np.array(part_vert,dtype='int32')
acc = compute_purity(C_metis_pyg, Cgt, nc)
print('\nAccuracy Metis PyG :',acc)

# Compute non-linear dim reduction
start = time.time()
[X,Y,Z] = nldr_visualization(nx.to_numpy_array(G_nx))
print('Time(sec): %.3f' % (time.time()-start) )
print(X.shape)

# 2D Visualization
plt.figure(3)
plt.scatter(X, Y, c=C_metis_pyg, s=3, color=pyplot.jet())
plt.show()

# 3D Visualization
import plotly.graph_objects as go
data = go.Scatter3d(x=X, y=Y, z=Z, mode='markers', marker=dict(size=1, color=C_metis_pyg, colorscale='jet', opacity=1)) # data as points
fig = go.Figure(data=[data])
fig.update_layout(margin=dict(l=0, r=0, b=0, t=30, pad=0)) # tight layout but t=25 required for showing title
fig.update_layout(autosize=False, width=600, height=600, title_text="3D visualization of USPS image graph") # figure size and title
fig.update_layout(scene = dict(zaxis = dict(showgrid = True, showticklabels = False), zaxis_title = ' ') ) # no range values, no axis name, grid on
fig.update_layout(scene = dict(yaxis = dict(showgrid = True, showticklabels = False), yaxis_title = ' ') ) # no range values, no axis name, grid on
fig.update_layout(scene = dict(xaxis = dict(showgrid = True, showticklabels = False), xaxis_title = ' ') ) # no range values, no axis name, grid on
fig.layout.scene.aspectratio = {'x':1, 'y':1, 'z':1}
fig.show()

"""Граф изображений USPS состоит из изображений рукописных цифр (от 0 до 9). При разбиении графа с помощью алгоритма Metis можно ожидать, что изображения одной цифры будут сгруппированы в отдельные кластеры.
При визуализации кластеров с использованием различных цветов можно заметить, как хорошо алгоритм Metis разделяет различные классы. Кластеры четко отделены друг от друга и соответствуют истинным классам, что может свидетельствовать о высокой эффективности алгоритма.

# NCut
"""

# Load four-circle dataset
mat = scipy.io.loadmat('datasets/four_circles.mat')
X = mat['X']
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print('(n,d,nc:',n,d,nc)

plt.figure(1)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, color=pyplot.jet())
plt.title('Ground truth communities of four concentric circles')
plt.show()

# Run standard/linear k-means with EM approach
Theta = np.ones(n) # Same weight for each data
# Compute linear Kernel for standard K-Means
Ker = construct_kernel(X, 'linear')
# Standard K-Means
C_kmeans, En_kmeans = compute_kernel_kmeans_EM(nc, Ker, Theta, 10)
# Plot
plt.figure(2)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans)
plt.title('Standard K-Means solution.\nAccuracy= ' + str(compute_purity(C_kmeans,Cgt,nc)) +
         ', Energy= ' + str(En_kmeans))
plt.show()

# Run kernel/non-linear k-means with spectral approach
Ker = construct_kernel(X, 'kNN_gaussian', 100)
# Kernel K-Means with Spectral approach
C_kmeans, En_kmeans = compute_kernel_kmeans_spectral(nc, Ker, Theta)
# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_kmeans, color=pyplot.jet())
plt.title('Kernel K-Means solution with Spectral.\nAccuracy= ' +
          str(compute_purity(C_kmeans,Cgt,nc)) + ', Energy= ' + str(En_kmeans))
plt.show()

"""Примените спектральный NCut с различными значениями k к графу k-NN

Поэкспериментируйте со следующими значениями k: {5, 10, 20, 40, 80}.

Объясняйте, что происходит, когда k мало, что приводит к разреженным графам, и когда k велико, что приводит к плотно связным графам.

"""

# Values of k to experiment with
k_values = [5, 10, 20, 40, 80]

for k in k_values:
    # Construct the k-NN graph
    W = construct_knn_graph(X, k, 'euclidean_zelnik_perona')

    # Run NCut
    C_ncut, acc = compute_ncut(W, Cgt, nc)

    # Plot the results
    plt.figure()
    size_vertex_plot = 10
    plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(X.shape[0]), c=C_ncut, cmap='jet')
    plt.title(f'NCut solution for k={k}. Accuracy= {compute_purity(C_ncut,Cgt,nc)}')
    plt.show()

"""Объяснение результатов

Для k=5: Мы увидим разреженный граф с ошибками в кластеризации. Сообщества плохо определены.

Для k=10 и k=20: Граф начинает становиться более связанным. Кластеризация улучшается по сравнению с k=5.

Для k=40 идеальное предсказание.

Для k=80 появляются "шумные" кластеры из-за смешивания точек из разных сообществ

# PCut

### Two-moon dataset
"""

# Load raw data images
mat = scipy.io.loadmat('datasets/two_moons.mat')
X = mat['X']
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print(n,d,nc)

# Plot
plt.figure(1)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, cmap='jet')
plt.title('Visualization of the two-moon datase with 2 classes, Data Dimentionality is 100')
plt.show()

"""Определите лушее значение k для графа k-NN в спектральном методе NCut

Какое значение k дает наилучшие результаты кластеризации?

"""

# Define a range of k values to test
k_values = range(10, 20)  # Adjust this range based on your needs
best_k = None
best_accuracy = 0

for k in k_values:
    # Construct the k-NN graph
    W = construct_knn_graph(X, k, 'euclidean')

    # Run NCut
    C_ncut, _ = compute_ncut(W, Cgt, nc)

    # Compute accuracy
    accuracy = compute_purity(C_ncut, Cgt, nc)

    print(f'k={k}, Accuracy={accuracy}')

    # Update best_k if current accuracy is better
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_k = k

print(f'Best k: {best_k} with Accuracy: {best_accuracy}')

# Run NCut
k = 13
W = construct_knn_graph(X, k, 'euclidean')
C_ncut, _ = compute_ncut(W, Cgt, nc)

# Plot
plt.figure(2)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_ncut, cmap='jet')
plt.title('NCut solution. Accuracy= ' +
          str(compute_purity(C_ncut, Cgt, nc))[:6] )
plt.show()

"""Наилучший резуьтат при k=13.

Оцените метод PCut с различными значениями k для графа k-NN.

Какое значение k дает наиболее эффективный результат кластеризации?

Кроме того, какой диапазон значений k обеспечивает лучшую эффективность кластеризации?
"""

# Define a range of k values to test
k_values = range(5, 26)  # Testing k from 1 to 20
best_k = None
best_accuracy = 0

for k in k_values:
    # Construct the k-NN graph
    W = construct_knn_graph(X, k, 'euclidean')

    # Run PCut
    C_pcut, _ = compute_pcut(W, Cgt, nc, 2, 200)

    # Compute accuracy
    accuracy = compute_purity(C_pcut, Cgt, nc)

    print(f'k={k}, Accuracy={accuracy}')


    # Update best_k if current accuracy is better
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_k = k

print(f'Best k: {best_k} with Accuracy: {best_accuracy}')

# Run PCut
k = 16
W = construct_knn_graph(X, k, 'euclidean')
C_pcut, _ = compute_pcut(W, Cgt, nc, 2, 200)

# Plot
plt.figure(3)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=C_pcut, cmap='jet')
plt.title('PCut solution. Accuracy= ' +
          str(compute_purity(C_pcut, Cgt, nc))[:6] )
plt.show()

"""k=16 дает наиболее эффективный результат кластеризации

Кроме того, диапазон значений k=[10,20] обеспечивает лучшую эффективность кластеризации.

Комментарий: Сравните спектральные методы NCut и PCut на двух реальных графиках

Запустите методы Spectral NCut и PCut на двух реальных графах и сравните их производительность.

### USPS image graph
"""

# Load USPS dataset
mat = scipy.io.loadmat('datasets/USPS.mat')
W = mat['W'] # 'scipy.sparse._csc.csc_matrix'
n = W.shape[0]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print(n,nc)

Cncut, acc = compute_ncut(W,Cgt,nc)
print('Ncut accuracy =',acc)

Cpcut, acc = compute_pcut(W,Cgt,nc,5,10)
print('Pcut accuracy =',acc)

"""### MIREX music graph"""

# Load USPS dataset
mat = scipy.io.loadmat('datasets/MIREX.mat')
W = mat['W'] # 'scipy.sparse._csc.csc_matrix'
n = W.shape[0]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print(n,nc)

Cncut, acc = compute_ncut(W,Cgt,nc)
print('Ncut accuracy =',acc)

Cpcut, acc = compute_pcut(W,Cgt,nc,0.5,400)
print('Pcut accuracy =',acc)

"""Таким образом, Pcut на реальных данных показывает лучший результат.

# Louvain Algorithm

### Two-moon dataset
"""

# Load two-circle dataset
mat = scipy.io.loadmat('datasets/two_circles.mat')
X = mat['X']
n = X.shape[0]
d = X.shape[1]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print('n,d,nc:',n,d,nc)

plt.figure(1)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Cgt, cmap='jet')
plt.title('Distribution of two circle distributions -- Non-linear data points')
plt.show()

"""Оцените метод Лувена

Сколько «оптимальных» кластеров определяет метод Лувена?
Какова точность кластеризации, достигнутая решением Лувена?
Можете ли вы объяснить высокую точность?


"""

# Run Louvain algorithm
W = construct_knn_graph(X, 50, 'euclidean_zelnik_perona')
Wnx = nx.from_numpy_array(W)
partition = community_louvain.best_partition(Wnx)
nc_louvain = len(np.unique( [partition[nodes] for nodes in partition.keys()] ))
n = len(Wnx.nodes())
print('nb_data:', n , ', nb_clusters=', nc_louvain)

# Extract clusters
Clouv = np.zeros([n])
clusters = []
k = 0
for com in set(partition.values()):
    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]
    Clouv[list_nodes] = k
    k += 1
    clusters.append(list_nodes)

# Accuracy
acc = compute_purity(Clouv,Cgt,nc_louvain)
print('accuracy_louvain=',acc,' with nb_clusters=',nc_louvain)

plt.figure(2)
size_vertex_plot = 10
plt.scatter(X[:,0], X[:,1], s=size_vertex_plot*np.ones(n), c=Clouv, cmap='jet')
plt.title('Louvain solution')
plt.show()

"""14 кластеров избыточные для двух окружностей (где ожидается два класса), это может быть результатом того, что алгоритм разбивает данные на более мелкие группы.

### USPS dataset

Compare the Louvain and spectral NCut solutions

Compare the clustering results of the Louvain method with those of the spectral NCut technique, using the same number of clusters.

How does the performance of the Spectral NCut technique change as the number of clusters increases?
"""

# Load USPS dataset
mat = scipy.io.loadmat('datasets/USPS.mat')
W = mat['W']
n = W.shape[0]
Cgt = mat['Cgt']-1; Cgt=Cgt.squeeze()
nc = len(np.unique(Cgt))
print('n,nc:',n,nc)

# Random partitionning
Crand = np.random.randint(0,nc,[n])
acc = compute_purity(Crand,Cgt,nc)
print('Random solution:', str(acc)[:5])

# Run NCut
Cncut, acc = compute_ncut(W,Cgt,nc)
print('NCut solution:', str(acc)[:5])

# Run Louvain
Wnx = nx.from_numpy_array(W.toarray())
partition = community_louvain.best_partition(Wnx)
nc_louvain = len(np.unique( [partition[nodes] for nodes in partition.keys()] ))
n = len(Wnx.nodes())
print('nb_data:', n , ', nb_clusters=', nc_louvain)

# Extract clusters
Clouv = np.zeros([n])
clusters = []
k = 0
for com in set(partition.values()):
    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]
    Clouv[list_nodes] = k
    k += 1
    clusters.append(list_nodes)

# Accuracy
acc = compute_purity(Clouv,Cgt,nc_louvain)
print('Louvain solution ',str(acc)[:5],' with nb_clusters=',nc_louvain)

# Run NCut with the number of clusters found by Louvain
Cncut, acc = compute_ncut(W,Cgt,nc_louvain)
print('NCut solution:',str(acc)[:5],' with nb_clusters=',nc_louvain)

"""Алгоритм Лувена разделил на большее количество кластеров. Для значения nc_louvain качество СNCut повысилось."""

