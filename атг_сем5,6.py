# -*- coding: utf-8 -*-
"""АТГ_сем5,6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYAJOOUAAJQkgZW_dd4_H8ohZ0Pa57Eg

# 5 задание

## 1.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T
import random

import networkx as nx
import random
import matplotlib.pyplot as plt
import numpy as np

def calculate_ati(G):
    """Расчет агрегированного топологического индекса (ATI)"""
    if len(G.nodes()) == 0:
        return 0

    # Метрики для расчета ATI
    degree_centrality = nx.degree_centrality(G)
    closeness_centrality = nx.closeness_centrality(G)
    betweenness_centrality = nx.betweenness_centrality(G)

    # Веса для метрик (можно настроить)
    w1, w2, w3 = 0.4, 0.3, 0.3

    # Расчет ATI как взвешенной суммы средних значений метрик
    ati = (w1 * np.mean(list(degree_centrality.values())) +
           w2 * np.mean(list(closeness_centrality.values())) +
           w3 * np.mean(list(betweenness_centrality.values())))

    return ati

def create_seed_graphs():
    """Создает набор графов-затравок."""
    seeds = {}

    # Затравка 1: треугольник
    G1 = nx.Graph()
    G1.add_edges_from([(0, 1), (1, 2), (2, 0)])
    seeds['triangle'] = G1

    # Затравка 2: звезда
    G2 = nx.Graph()
    G2.add_edges_from([(0, i) for i in range(1, 4)])
    seeds['star'] = G2

    # Затравка 3: путь
    G3 = nx.Graph()
    G3.add_edges_from([(0, 1), (1, 2)])
    seeds['path'] = G3

    return seeds

def evolve_graph_with_ati(initial_seed, levels, num_operations):
    """Эволюция графа с отслеживанием ATI"""
    G = nx.Graph()
    G.add_nodes_from(initial_seed.nodes)
    G.add_edges_from(initial_seed.edges)

    seeds = create_seed_graphs()
    trajectory = []  # Список для хранения состояний графа
    ati_values = []  # Список для хранения значений ATI

    for level in range(levels):
        for _ in range(num_operations):
            # Выполняем операции модификации графа
            operation = random.choice(["add_vertex", "add_edge", "replace_vertex"])

            if operation == "add_vertex":
                new_node = max(G.nodes) + 1 if len(G.nodes) > 0 else 0
                existing_node = random.choice(list(G.nodes))
                weight = 1 + level / levels
                G.add_node(new_node)
                G.add_edge(new_node, existing_node, weight=weight)

            elif operation == "add_edge":
                nodes = list(G.nodes)
                if len(nodes) > 1:
                    u, v = random.sample(nodes, 2)
                    if not G.has_edge(u, v):
                        weight = 1 + level / levels
                        G.add_edge(u, v, weight=weight)

            elif operation == "replace_vertex" and len(G.nodes) > 0:
                vertex = random.choice(list(G.nodes))
                seed_graph = random.choice(list(seeds.values()))
                neighbors = list(G.neighbors(vertex))
                G.remove_node(vertex)

                # Добавляем затравку
                mapping = {}
                for n in seed_graph.nodes:
                    new_node = max(G.nodes) + 1 if n != 0 else vertex
                    G.add_node(new_node)
                    mapping[n] = new_node

                for u, v in seed_graph.edges:
                    weight = 1 + level / levels
                    G.add_edge(mapping[u], mapping[v], weight=weight)

                # Восстанавливаем связи
                for neighbor in neighbors:
                    G.add_edge(vertex, neighbor, weight=1 + level / levels)

        # Сохраняем текущее состояние и ATI
        trajectory.append(G.copy())
        ati_values.append(calculate_ati(G))

        # Проверка на минимальное количество вершин
        if len(G.nodes) >= 500:
            break

    return G, trajectory, ati_values

# Создаем и эволюционируем граф
initial_seed = nx.complete_graph(200)
levels = 25
num_operations = 10
final_graph, trajectory, ati_values = evolve_graph_with_ati(initial_seed, levels, num_operations)

# Визуализация изменения ATI
plt.figure(figsize=(10, 6))
plt.plot(ati_values, marker='o')
plt.title('Изменение ATI в процессе эволюции графа')
plt.xlabel('Шаг эволюции')
plt.ylabel('ATI')
plt.grid(True)
plt.show()

# Визуализация финального графа
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(final_graph)
edges = final_graph.edges(data=True)
weights = [edge[2].get('weight', 1) for edge in edges]
nx.draw(final_graph, pos, with_labels=False, width=weights, node_size=10)
plt.title("Финальный эволюционный граф")
plt.show()

print(f"Количество вершин в финальном графе: {final_graph.number_of_nodes()}")
print(f"Количество сохраненных состояний: {len(trajectory)}")
print(f"Финальное значение ATI: {ati_values[-1]:.4f}")

import networkx as nx
import random
import numpy as np
from typing import List, Set, Tuple
import matplotlib.pyplot as plt
from collections import defaultdict

class GraphDataset:
    def __init__(self, graphs):
        self.graphs = graphs  # Сохраняем список графов
        self.chain_covers = []
        self.optimal_covers = []
        self.num_graphs = len(graphs)

    def find_chain_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит покрытие графа непересекающимися цепями"""
        remaining_vertices = set(G.nodes())
        chains = []

        while remaining_vertices:
            # Начинаем новую цепь
            current_chain = []
            start_vertex = random.choice(list(remaining_vertices))
            current_chain.append(start_vertex)
            remaining_vertices.remove(start_vertex)

            while True:
                # Ищем следующую вершину для цепи
                current = current_chain[-1]
                neighbors = set(G.neighbors(current)) & remaining_vertices

                if not neighbors:
                    break

                next_vertex = random.choice(list(neighbors))
                current_chain.append(next_vertex)
                remaining_vertices.remove(next_vertex)

            chains.append(current_chain)

        return chains

    def find_optimal_cover(self, G: nx.Graph) -> List[List[int]]:
        """Находит оптимальное покрытие графа цепями (жадный алгоритм)"""
        remaining_edges = set(G.edges())
        chains = []

        while remaining_edges:
            # Находим самый длинный путь в оставшихся рёбрах
            current_chain = self._find_longest_path(G, remaining_edges)
            chains.append(current_chain)

            # Удаляем использованные рёбра
            for i in range(len(current_chain)-1):
                edge = (current_chain[i], current_chain[i+1])
                rev_edge = (current_chain[i+1], current_chain[i])
                if edge in remaining_edges:
                    remaining_edges.remove(edge)
                if rev_edge in remaining_edges:
                    remaining_edges.remove(rev_edge)

        # Добавляем изолированные вершины как отдельные цепи
        isolated_vertices = [v for v in G.nodes() if G.degree(v) == 0]
        for vertex in isolated_vertices:
            chains.append([vertex])

        return chains

    def _find_longest_path(self, G: nx.Graph, available_edges: Set[Tuple[int, int]]) -> List[int]:
        """Находит самый длинный путь в графе, используя только доступные рёбра"""
        if not available_edges:
            return []

        # Начинаем с произвольного ребра
        start_edge = random.choice(list(available_edges))
        path = list(start_edge)

        while True:
            # Пытаемся расширить путь с обоих концов
            extended = False

            # Пробуем расширить в начало
            current = path[0]
            for neighbor in G.neighbors(current):
                edge = (neighbor, current) if (neighbor, current) in available_edges else (current, neighbor)
                if edge in available_edges and neighbor not in path:
                    path.insert(0, neighbor)
                    extended = True
                    break

            # Пробуем расширить в конец
            current = path[-1]
            for neighbor in G.neighbors(current):
                edge = (current, neighbor) if (current, neighbor) in available_edges else (neighbor, current)
                if edge in available_edges and neighbor not in path:
                    path.append(neighbor)
                    extended = True
                    break

            if not extended:
                break

        return path
    # Анализ покрытия цепями

    def analyze_cover(self, G: nx.Graph, chains: List[List[int]]):
        """Анализирует покрытие цепями: возвращает количество цепей и общий вес покрытия."""
        total_weight = 0
        for chain in chains:
            for u, v in zip(chain, chain[1:]):
                if G.has_edge(u, v):
                    total_weight += G[u][v].get('weight', 1)  # Учитываем вес ребра, по умолчанию 1
                else:
                    print(f"Warning: Edge ({u}, {v}) not found in the graph.")
        return len(chains), total_weight


    # Визуализация покрытия

    def visualize_cover(self, G: nx.Graph, chains: List[List[int]], title: str):
        pos = nx.spring_layout(G)
        plt.figure(figsize=(10, 6))

        for chain in chains:
            edges = [(chain[i], chain[i + 1]) for i in range(len(chain) - 1)]
            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color='r', width=2)

        nx.draw_networkx_nodes(G, pos, node_size=5, node_color='b')
        plt.title(title)
        plt.show()

    def generate_dataset(self):
        """Генерирует датасет с покрытиями для всех графов"""
        self.chain_covers = []
        self.optimal_covers = []
        for G in self.graphs:
            chain_cover = self.find_chain_cover(G)
            optimal_cover = self.find_optimal_cover(G)
            self.chain_covers.append(chain_cover)
            self.optimal_covers.append(optimal_cover)

# Создаём и анализируем датасет
dataset = GraphDataset(trajectory)
dataset.generate_dataset()

final_graph = dataset.graphs[0]
chain_cover = dataset.find_chain_cover(final_graph)
optimal_cover = dataset.find_optimal_cover(final_graph)

chain_stats = dataset.analyze_cover(final_graph, chain_cover)
optimal_stats = dataset.analyze_cover(final_graph, optimal_cover)

print(f"Цепное покрытие: {chain_stats[0]} цепей, общий вес: {chain_stats[1]:.2f}")
print(f"Оптимальное покрытие: {optimal_stats[0]} цепей, общий вес: {optimal_stats[1]:.2f}")

# Визуализация
visualize_cover(final_graph, chain_cover, "Цепное покрытие")
visualize_cover(final_graph, optimal_cover, "Оптимальное покрытие")

# График изменения ATI
plt.figure(figsize=(10, 6))
plt.plot(ati_values, marker='o', label='ATI')
plt.title('Изменение ATI в процессе эволюции графа')
plt.xlabel('Шаг эволюции')
plt.ylabel('ATI')
plt.grid(True)
plt.legend()
plt.show()

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import from_networkx
import torch_geometric.transforms as T

class GraphCoverPredictor(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):
        super(GraphCoverPredictor, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, hidden_dim)

        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x, edge_index, batch):
        # Graph convolution layers
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))

        # Global pooling
        x = global_mean_pool(x, batch)

        # MLP для финального предсказания
        return self.mlp(x)

class ChainCoverTrainer:
    def __init__(self, dataset):
        self.dataset = dataset
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = GraphCoverPredictor().to(self.device)

    def prepare_data(self):
        """Подготовка данных для обучения"""
        data_list = []

        for i, (G, optimal_cover) in enumerate(zip(self.dataset.graphs, self.dataset.optimal_covers)):
            # Добавляем одинаковый атрибут для всех рёбер
            for (u, v) in G.edges():
                G[u][v]['weight'] = 1.0

            # Преобразуем networkx граф в PyTorch Geometric формат
            pyg_graph = from_networkx(G)

            # Добавляем признаки вершин (используем degree как простой признак)
            degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
            pyg_graph.x = degrees

            # Целевое значение - количество цепей в оптимальном покрытии
            pyg_graph.y = torch.tensor([len(optimal_cover)], dtype=torch.float)

            data_list.append(pyg_graph)

        return data_list

    def train(self, num_epochs=10):
        """Обучение модели"""
        data_list = self.prepare_data()

        # Разделение на train/val
        train_size = int(0.8 * len(data_list))
        train_dataset = data_list[:train_size]
        val_dataset = data_list[train_size:]

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32)

        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()

        best_val_loss = float('inf')

        # Списки для хранения истории обучения
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_mae': [],
            'val_mae': []
        }

        for epoch in range(num_epochs):
            # Обучение
            self.model.train()
            total_loss = 0
            train_predictions = []
            train_targets = []

            for batch in train_loader:
                batch = batch.to(self.device)
                optimizer.zero_grad()

                out = self.model(batch.x, batch.edge_index, batch.batch)
                loss = criterion(out, batch.y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

                train_predictions.extend(out.cpu().detach().numpy())
                train_targets.extend(batch.y.cpu().numpy())

            # Вычисляем MAE для тренировочной выборки
            train_mae = np.mean([abs(p - t) for p, t in zip(train_predictions, train_targets)])

            # Валидация
            self.model.eval()
            val_loss = 0
            val_predictions = []
            val_targets = []

            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    out = self.model(batch.x, batch.edge_index, batch.batch)

                    val_loss += criterion(out, batch.y).item()
                    val_predictions.extend(out.cpu().numpy())
                    val_targets.extend(batch.y.cpu().numpy())

            val_mae = np.mean([abs(p - t) for p, t in zip(val_predictions, val_targets)])

            # Сохраняем метрики
            history['train_loss'].append(total_loss/len(train_loader))
            history['val_loss'].append(val_loss/len(val_loader))
            history['train_mae'].append(train_mae)
            history['val_mae'].append(val_mae)

            if (epoch + 1) % 10 == 0:
                print(f'Epoch {epoch+1}/{num_epochs}:')
                print(f'Train Loss: {history["train_loss"][-1]:.4f}')
                print(f'Val Loss: {history["val_loss"][-1]:.4f}')
                print(f'Train MAE: {train_mae:.4f}')
                print(f'Val MAE: {val_mae:.4f}')

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(self.model.state_dict(), 'best_model.pt')

        # Визуализация процесса обучения
        plt.figure(figsize=(12, 5))

        # График функции потерь
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss (MSE)')
        plt.title('Learning Curves - Loss')
        plt.legend()
        plt.grid(True)

        # График MAE
        plt.subplot(1, 2, 2)
        plt.plot(history['train_mae'], label='Train MAE')
        plt.plot(history['val_mae'], label='Validation MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.title('Learning Curves - MAE')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.show()

        return history

    def predict(self, G):
        """Предсказание количества цепей для нового графа"""
        self.model.eval()
        for (u, v) in G.edges():
            G[u][v]['weight'] = 1.0

        # Подготовка графа
        pyg_graph = from_networkx(G)
        degrees = torch.tensor([[d] for n, d in G.degree()], dtype=torch.float)
        pyg_graph.x = degrees

        # Создаем batch из одного графа
        loader = DataLoader([pyg_graph], batch_size=1)
        batch = next(iter(loader)).to(self.device)

        with torch.no_grad():
            pred = self.model(batch.x, batch.edge_index, batch.batch)

        return int(pred.item() + 0.5)  # округляем до ближайшего целого

# Создаем тренера и обучаем модель
trainer = ChainCoverTrainer(dataset)
trainer.train(num_epochs=50)

# Создаем и эволюционируем граф
initial_seed1 = nx.complete_graph(200)
levels1 = 5
num_operations1 = 1
test_graph, _, _ = evolve_graph_with_ati(initial_seed1, levels1, num_operations1)
predicted_chains = trainer.predict(test_graph)
actual_chains = len(dataset.find_optimal_cover(test_graph))

print("\nРезультаты тестирования:")
print(f"Предсказанное количество цепей: {predicted_chains}")
print(f"Фактическое количество цепей: {actual_chains}")
print(f"Отклонение: {abs(predicted_chains - actual_chains)}")

"""# 6 задание"""

import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import random
import time

def find_graph_center(graph):
    """
    Итерационный алгоритм для нахождения центра графа и оценки устойчивости.
    """
    graph_copy = graph.copy()
    integrity = []
    iterations = 0

    while nx.is_connected(graph_copy):
        # Найти центр графа (вершину с минимальным максимальным расстоянием до других)
        eccentricities = nx.eccentricity(graph_copy)
        center = min(eccentricities, key=eccentricities.get)

        # Удаляем центр и проверяем связность
        graph_copy.remove_node(center)
        iterations += 1
        integrity.append(nx.is_connected(graph_copy))

    return iterations, integrity

def plot_graph_metrics(metrics, title, xlabel, ylabel):
    """Построение графика изменений показателей графа."""
    plt.figure(figsize=(10, 6))
    plt.plot(metrics, marker='o')
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid()
    plt.show()

# Анализ графов из траектории
iterations_list = []
integrity_list = []

for idx, graph in enumerate(trajectory):
    iterations, integrity = find_graph_center(graph)
    iterations_list.append(iterations)
    integrity_list.append(sum(integrity))

# Построение графиков
plot_graph_metrics(iterations_list, "Изменение количества итераций до разрушения", "Шаги", "Количество итераций")
plot_graph_metrics(integrity_list, "Изменение целостности графов", "Шаги", "Целостность")

def create_large_graph(trajectory):
    """Создание объединенного графа из траектории графов."""
    large_graph = nx.Graph()
    for graph in trajectory:
        large_graph = nx.compose(large_graph, graph)
    return large_graph

# Создание и анализ большого графа
large_graph = create_large_graph(trajectory)
large_iterations, large_integrity = find_graph_center(large_graph)

# Построение графиков для большого графа
plot_graph_metrics([large_iterations], "Количество итераций до разрушения для большого графа", "", "Количество итераций")
plot_graph_metrics([sum(large_integrity)], "Целостность большого графа", "", "Целостность")

# Анализ графов из траектории
print(iterations_list)
print(large_iterations)
print(integrity_list)
print(large_integrity)

# Визуализация "большого" графа
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(large_graph)
edges = large_graph.edges(data=True)
weights = [edge[2].get('weight', 1) for edge in edges]
nx.draw(large_graph, pos, with_labels=False, width=weights, node_size=10)
plt.title("Финальный большой граф")
plt.show()

def modify_edge_weights(graph, min_weight=-10, max_weight=10):
    """Случайным образом изменяет веса рёбер графа."""
    for u, v in graph.edges():
        graph[u][v]['weight'] = random.uniform(min_weight, max_weight)
    return graph

# Замена весов рёбер на отрицательные
modified_graph = modify_edge_weights(large_graph)

try:
    modified_iterations, modified_integrity = find_graph_center(modified_graph)
    print("Алгоритм успешно обработал граф с отрицательными весами.")
except Exception as e:
    print(f"Ошибка при обработке графа с отрицательными весами: {e}")

def floyd_warshall_algorithm(graph):
    """Реализация алгоритма Флойда-Уоршелла для поиска кратчайших путей."""
    nodes = list(graph.nodes)
    n = len(nodes)
    dist = {node: {node: float('inf') for node in nodes} for node in nodes}

    for node in nodes:
        dist[node][node] = 0

    for u, v, data in graph.edges(data=True):
        weight = data.get('weight', 1)
        dist[u][v] = weight
        dist[v][u] = weight

    for k in nodes:
        for i in nodes:
            for j in nodes:
                if dist[i][j] > dist[i][k] + dist[k][j]:
                    dist[i][j] = dist[i][k] + dist[k][j]

    return dist

def bellman_ford_algorithm(graph, source):
    """Реализация алгоритма Беллмана-Форда для поиска кратчайших путей."""
    nodes = list(graph.nodes)
    dist = {node: float('inf') for node in nodes}
    dist[source] = 0

    for _ in range(len(nodes) - 1):
        for u, v, data in graph.edges(data=True):
            weight = data.get('weight', 1)
            if dist[u] + weight < dist[v]:
                dist[v] = dist[u] + weight

    return dist

def compare_algorithms(graph, source):
    """Сравнение времени выполнения алгоритмов."""
    # Алгоритм Дейкстры
    start_time = time.time()
    nx.single_source_dijkstra_path_length(graph, source)
    dijkstra_time = time.time() - start_time

    # Алгоритм Флойда-Уоршелла
    start_time = time.time()
    floyd_warshall_algorithm(graph)
    floyd_time = time.time() - start_time

    # Алгоритм Беллмана-Форда
    start_time = time.time()
    bellman_ford_algorithm(graph, source)
    bellman_time = time.time() - start_time

    return dijkstra_time, floyd_time, bellman_time

for u, v, data in large_graph.edges(data=True):
    print(f"Edge ({u}, {v}) with weight: {data.get('weight')}")

# Сравнение алгоритмов на большом графе
source_node = list(large_graph.nodes)[0]
dijkstra_time, floyd_time, bellman_time = compare_algorithms(large_graph, source_node)

print(f"Время выполнения алгоритма Дейкстры: {dijkstra_time:.6f} секунд")
print(f"Время выполнения алгоритма Флойда-Уоршелла: {floyd_time:.6f} секунд")
print(f"Время выполнения алгоритма Беллмана-Форда: {bellman_time:.6f} секунд")

# Импорт библиотек
import networkx as nx
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Функция для извлечения признаков графа
def extract_graph_features(graph):
    features = {}
    features['num_nodes'] = graph.number_of_nodes()
    features['num_edges'] = graph.number_of_edges()
    features['density'] = nx.density(graph)
    features['avg_clustering'] = nx.average_clustering(graph)
    features['avg_degree_centrality'] = np.mean(list(nx.degree_centrality(graph).values()))
    features['avg_closeness_centrality'] = np.mean(list(nx.closeness_centrality(graph).values()))
    features['avg_betweenness_centrality'] = np.mean(list(nx.betweenness_centrality(graph).values()))
    return features

# Создание набора данных на основе траектории графов
def create_dataset(trajectory, integrity_list):
    X = []
    y = []
    for graph, integrity in zip(trajectory, integrity_list):
        X.append(list(extract_graph_features(graph).values()))
        y.append(integrity)
    return np.array(X), np.array(y)

# Генерация данных
X, y = create_dataset(trajectory, integrity_list)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Создание и обучение модели Random Forest
model = RandomForestRegressor(random_state=42)
model.fit(X_train, y_train)

# Прогнозирование и оценка модели
y_pred = model.predict(X_test)

# Метрики качества
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Среднеквадратичная ошибка (MSE): {mse:.4f}")
print(f"Коэффициент детерминации (R^2): {r2:.4f}")